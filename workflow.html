<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>geoengine.workflow API documentation</title>
<meta name="description" content="A workflow representation and methods on workflows" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>geoengine.workflow</code></h1>
</header>
<section id="section-intro">
<p>A workflow representation and methods on workflows</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
A workflow representation and methods on workflows
&#39;&#39;&#39;

from __future__ import annotations

import asyncio
import json
import urllib.parse
from io import BytesIO
from logging import debug
from os import PathLike
from typing import Any, AsyncIterator, Dict, List, Optional, Union, Type, cast
from uuid import UUID

import geopandas as gpd
import pandas as pd
import numpy as np
import rasterio.io
import requests as req
import rioxarray
from PIL import Image
from owslib.util import Authentication, ResponseWrapper
from owslib.wcs import WebCoverageService
# TODO: can be imported directly from `typing` with python &gt;= 3.8
from typing_extensions import TypedDict
from vega import VegaLite
import websockets
import websockets.client
import xarray as xr
import pyarrow as pa

from geoengine import api
from geoengine.auth import get_session
from geoengine.colorizer import Colorizer
from geoengine.error import GeoEngineException, InputException, MethodNotCalledOnPlotException, \
    MethodNotCalledOnRasterException, MethodNotCalledOnVectorException, TypeException, check_response_for_error, \
    InvalidUrlException
from geoengine import backports
from geoengine.types import GeoTransform, ProvenanceEntry, QueryRectangle, ResultDescriptor, TimeInterval
from geoengine.tasks import Task, TaskId


# TODO: Define as recursive type when supported in mypy: https://github.com/python/mypy/issues/731
JsonType = Union[Dict[str, Any], List[Any], int, str, float, bool, Type[None]]

Axis = TypedDict(&#39;Axis&#39;, {&#39;title&#39;: str})
Bin = TypedDict(&#39;Bin&#39;, {&#39;binned&#39;: bool, &#39;step&#39;: float})
Field = TypedDict(&#39;Field&#39;, {&#39;field&#39;: str})
DatasetIds = TypedDict(&#39;DatasetIds&#39;, {&#39;upload&#39;: UUID, &#39;dataset&#39;: UUID})
Values = TypedDict(&#39;Values&#39;, {&#39;binStart&#39;: float, &#39;binEnd&#39;: float, &#39;Frequency&#39;: int})
X = TypedDict(&#39;X&#39;, {&#39;field&#39;: Field, &#39;bin&#39;: Bin, &#39;axis&#39;: Axis})
X2 = TypedDict(&#39;X2&#39;, {&#39;field&#39;: Field})
Y = TypedDict(&#39;Y&#39;, {&#39;field&#39;: Field, &#39;type&#39;: str})
Encoding = TypedDict(&#39;Encoding&#39;, {&#39;x&#39;: X, &#39;x2&#39;: X2, &#39;y&#39;: Y})
VegaSpec = TypedDict(&#39;VegaSpec&#39;, {&#39;$schema&#39;: str, &#39;data&#39;: List[Values], &#39;mark&#39;: str, &#39;encoding&#39;: Encoding})


class WorkflowId:
    &#39;&#39;&#39;
    A wrapper around a workflow UUID
    &#39;&#39;&#39;

    __workflow_id: UUID

    def __init__(self, workflow_id: UUID) -&gt; None:
        self.__workflow_id = workflow_id

    @classmethod
    def from_response(cls, response: api.WorkflowId) -&gt; WorkflowId:
        &#39;&#39;&#39;
        Create a `WorkflowId` from an http response
        &#39;&#39;&#39;
        if &#39;id&#39; not in response:
            raise TypeError(&#39;Response does not contain a workflow id.&#39;)
        return WorkflowId(UUID(response[&#39;id&#39;]))

    def __str__(self) -&gt; str:
        return str(self.__workflow_id)

    def __repr__(self) -&gt; str:
        return str(self)


class Workflow:
    &#39;&#39;&#39;
    Holds a workflow id and allows querying data
    &#39;&#39;&#39;

    __workflow_id: WorkflowId
    __result_descriptor: ResultDescriptor

    def __init__(self, workflow_id: WorkflowId) -&gt; None:
        self.__workflow_id = workflow_id
        self.__result_descriptor = self.__query_result_descriptor()

    def __str__(self) -&gt; str:
        return str(self.__workflow_id)

    def __repr__(self) -&gt; str:
        return repr(self.__workflow_id)

    def __query_result_descriptor(self, timeout: int = 60) -&gt; ResultDescriptor:
        &#39;&#39;&#39;
        Query the metadata of the workflow result
        &#39;&#39;&#39;

        session = get_session()

        response = req.get(
            f&#39;{session.server_url}/workflow/{self.__workflow_id}/metadata&#39;,
            headers=session.auth_header,
            timeout=timeout
        ).json()

        debug(response)

        return ResultDescriptor.from_response(response)

    def get_result_descriptor(self) -&gt; ResultDescriptor:
        &#39;&#39;&#39;
        Return the metadata of the workflow result
        &#39;&#39;&#39;

        return self.__result_descriptor

    def workflow_definition(self, timeout: int = 60) -&gt; Dict[str, Any]:
        &#39;&#39;&#39;Return the workflow definition for this workflow&#39;&#39;&#39;

        session = get_session()

        response = req.get(
            f&#39;{session.server_url}/workflow/{self.__workflow_id}&#39;,
            headers=session.auth_header,
            timeout=timeout
        ).json()

        return response

    def __get_wfs_url(self, bbox: QueryRectangle) -&gt; str:
        &#39;&#39;&#39;Build a WFS url from a workflow and a `QueryRectangle`&#39;&#39;&#39;

        session = get_session()

        params = {
            &#39;service&#39;: &#39;WFS&#39;,
            &#39;version&#39;: &#34;2.0.0&#34;,
            &#39;request&#39;: &#39;GetFeature&#39;,
            &#39;outputFormat&#39;: &#39;application/json&#39;,
            &#39;typeNames&#39;: f&#39;{self.__workflow_id}&#39;,
            &#39;bbox&#39;: bbox.bbox_str,
            &#39;time&#39;: bbox.time_str,
            &#39;srsName&#39;: bbox.srs,
            &#39;queryResolution&#39;: str(bbox.spatial_resolution)
        }

        wfs_url = req.Request(
            &#39;GET&#39;, url=f&#39;{session.server_url}/wfs/{self.__workflow_id}&#39;, params=params).prepare().url

        debug(f&#39;WFS URL:\n{wfs_url}&#39;)

        if not wfs_url:
            raise InvalidUrlException(&#39;Failed to build WFS URL for workflow {self.__workflow_id}.&#39;)
        return wfs_url

    def get_wfs_get_feature_curl(self, bbox: QueryRectangle) -&gt; str:
        &#39;&#39;&#39;Return the WFS url for a workflow and a `QueryRectangle` as a cURL command&#39;&#39;&#39;

        if not self.__result_descriptor.is_vector_result():
            raise MethodNotCalledOnVectorException()

        wfs_request = req.Request(
            &#39;GET&#39;,
            url=self.__get_wfs_url(bbox),
            headers=get_session().auth_header
        ).prepare()

        command = &#34;curl -X {method} -H {headers} &#39;{uri}&#39;&#34;
        headers_list = [f&#39;&#34;{k}: {v}&#34;&#39; for k, v in wfs_request.headers.items()]
        headers = &#34; -H &#34;.join(headers_list)
        return command.format(method=wfs_request.method, headers=headers, uri=wfs_request.url)

    def get_dataframe(self, bbox: QueryRectangle, timeout: int = 3600) -&gt; gpd.GeoDataFrame:
        &#39;&#39;&#39;
        Query a workflow and return the WFS result as a GeoPandas `GeoDataFrame`
        &#39;&#39;&#39;

        if not self.__result_descriptor.is_vector_result():
            raise MethodNotCalledOnVectorException()

        session = get_session()

        wfs_url = self.__get_wfs_url(bbox)

        data_response = req.get(wfs_url, headers=session.auth_header, timeout=timeout)

        check_response_for_error(data_response)

        data = data_response.json()

        def geo_json_with_time_to_geopandas(geo_json):
            &#39;&#39;&#39;
            GeoJson has no standard for time, so we parse the when field
            separately and attach it to the data frame as columns `start`
            and `end`.
            &#39;&#39;&#39;

            data = gpd.GeoDataFrame.from_features(geo_json)
            data = data.set_crs(bbox.srs, allow_override=True)

            start = [f[&#39;when&#39;][&#39;start&#39;] for f in geo_json[&#39;features&#39;]]
            end = [f[&#39;when&#39;][&#39;end&#39;] for f in geo_json[&#39;features&#39;]]

            # TODO: find a good way to infer BoT/EoT

            data[&#39;start&#39;] = gpd.pd.to_datetime(start, errors=&#39;coerce&#39;)
            data[&#39;end&#39;] = gpd.pd.to_datetime(end, errors=&#39;coerce&#39;)

            return data

        return geo_json_with_time_to_geopandas(data)

    def wms_get_map_as_image(self, bbox: QueryRectangle, colorizer: Colorizer) -&gt; Image:
        &#39;&#39;&#39;Return the result of a WMS request as a PIL Image&#39;&#39;&#39;

        wms_request = self.__wms_get_map_request(bbox, colorizer)
        response = req.Session().send(wms_request)

        check_response_for_error(response)

        return Image.open(BytesIO(response.content))

    def __wms_get_map_request(self,
                              bbox: QueryRectangle,
                              colorizer: Colorizer) -&gt; req.PreparedRequest:
        &#39;&#39;&#39;Return the WMS url for a workflow and a given `QueryRectangle`&#39;&#39;&#39;

        if not self.__result_descriptor.is_raster_result():
            raise MethodNotCalledOnRasterException()

        session = get_session()

        width = int((bbox.spatial_bounds.xmax - bbox.spatial_bounds.xmin) / bbox.spatial_resolution.x_resolution)
        height = int((bbox.spatial_bounds.ymax - bbox.spatial_bounds.ymin) / bbox.spatial_resolution.y_resolution)

        colorizer_colorizer_str = &#39;custom:&#39; + colorizer.to_json()

        params = {
            &#39;service&#39;: &#39;WMS&#39;,
            &#39;version&#39;: &#39;1.3.0&#39;,
            &#39;request&#39;: &#34;GetMap&#34;,
            &#39;layers&#39;: str(self),
            &#39;time&#39;: bbox.time_str,
            &#39;crs&#39;: bbox.srs,
            &#39;bbox&#39;: bbox.bbox_ogc_str,
            &#39;width&#39;: width,
            &#39;height&#39;: height,
            &#39;format&#39;: &#39;image/png&#39;,
            &#39;styles&#39;: colorizer_colorizer_str,
        }

        return req.Request(
            &#39;GET&#39;,
            url=f&#39;{session.server_url}/wms/{str(self)}&#39;,
            params=params,
            headers=session.auth_header
        ).prepare()

    def wms_get_map_curl(self, bbox: QueryRectangle, colorizer: Colorizer) -&gt; str:
        &#39;&#39;&#39;Return the WMS curl command for a workflow and a given `QueryRectangle`&#39;&#39;&#39;

        wms_request = self.__wms_get_map_request(bbox, colorizer)

        command = &#34;curl -X {method} -H {headers} &#39;{uri}&#39;&#34;
        headers_list = [f&#39;&#34;{k}: {v}&#34;&#39; for k, v in wms_request.headers.items()]
        headers = &#34; -H &#34;.join(headers_list)
        return command.format(method=wms_request.method, headers=headers, uri=wms_request.url)

    def plot_chart(self, bbox: QueryRectangle, timeout: int = 3600) -&gt; VegaLite:
        &#39;&#39;&#39;
        Query a workflow and return the plot chart result as a vega plot
        &#39;&#39;&#39;

        if not self.__result_descriptor.is_plot_result():
            raise MethodNotCalledOnPlotException()

        session = get_session()

        time = urllib.parse.quote(bbox.time_str)
        spatial_bounds = urllib.parse.quote(bbox.bbox_str)
        resolution = str(bbox.spatial_resolution)

        plot_url = f&#39;{session.server_url}/plot/{self}?bbox={spatial_bounds}&amp;crs={bbox.srs}&amp;time={time}&#39;\
            f&#39;&amp;spatialResolution={resolution}&#39;

        response = req.get(plot_url, headers=session.auth_header, timeout=timeout)

        check_response_for_error(response)

        response_json: JsonType = response.json()
        assert isinstance(response_json, Dict)

        vega_spec: VegaSpec = json.loads(response_json[&#39;data&#39;][&#39;vegaString&#39;])

        return VegaLite(vega_spec)

    def __request_wcs(
        self,
        bbox: QueryRectangle,
        timeout=3600,
        file_format: str = &#39;image/tiff&#39;,
        force_no_data_value: Optional[float] = None
    ) -&gt; ResponseWrapper:
        &#39;&#39;&#39;
        Query a workflow and return the coverage

        Parameters
        ----------
        bbox : A bounding box for the query
        timeout : HTTP request timeout in seconds
        file_format : The format of the returned raster
        force_no_data_value: If not None, use this value as no data value for the requested raster data. \
            Otherwise, use the Geo Engine will produce masked rasters.
        &#39;&#39;&#39;

        if not self.__result_descriptor.is_raster_result():
            raise MethodNotCalledOnRasterException()

        session = get_session()

        # TODO: properly build CRS string for bbox
        crs = f&#39;urn:ogc:def:crs:{bbox.srs.replace(&#34;:&#34;, &#34;::&#34;)}&#39;

        wcs_url = f&#39;{session.server_url}/wcs/{self.__workflow_id}&#39;
        wcs = WebCoverageService(
            wcs_url,
            version=&#39;1.1.1&#39;,
            auth=Authentication(auth_delegate=session.requests_bearer_auth()),
        )

        [resx, resy] = bbox.resolution_ogc

        no_data_value = &#34;&#34;
        if force_no_data_value is not None:
            no_data_value = str(float(force_no_data_value))

        return wcs.getCoverage(
            identifier=f&#39;{self.__workflow_id}&#39;,
            bbox=bbox.bbox_ogc,
            time=[urllib.parse.quote_plus(bbox.time_str)],
            format=file_format,
            crs=crs,
            resx=resx,
            resy=resy,
            timeout=timeout,
            nodatavalue=no_data_value,
        )

    def __get_wcs_tiff_as_memory_file(
        self,
        bbox: QueryRectangle,
        timeout=3600,
        force_no_data_value: Optional[float] = None
    ) -&gt; rasterio.io.MemoryFile:
        &#39;&#39;&#39;
        Query a workflow and return the raster result as a memory mapped GeoTiff

        Parameters
        ----------
        bbox : A bounding box for the query
        timeout : HTTP request timeout in seconds
        force_no_data_value: If not None, use this value as no data value for the requested raster data. \
            Otherwise, use the Geo Engine will produce masked rasters.
        &#39;&#39;&#39;

        response = self.__request_wcs(bbox, timeout, &#39;image/tiff&#39;, force_no_data_value).read()

        # response is checked via `raise_on_error` in `getCoverage` / `openUrl`

        memory_file = rasterio.io.MemoryFile(response)

        return memory_file

    def get_array(
        self,
        bbox: QueryRectangle,
        timeout=3600,
        force_no_data_value: Optional[float] = None
    ) -&gt; np.ndarray:
        &#39;&#39;&#39;
        Query a workflow and return the raster result as a numpy array

        Parameters
        ----------
        bbox : A bounding box for the query
        timeout : HTTP request timeout in seconds
        force_no_data_value: If not None, use this value as no data value for the requested raster data. \
            Otherwise, use the Geo Engine will produce masked rasters.
        &#39;&#39;&#39;

        with self.__get_wcs_tiff_as_memory_file(
            bbox,
            timeout,
            force_no_data_value
        ) as memfile, memfile.open() as dataset:
            array = dataset.read(1)

            return array

    def get_xarray(
        self,
        bbox: QueryRectangle,
        timeout=3600,
        force_no_data_value: Optional[float] = None
    ) -&gt; xr.DataArray:
        &#39;&#39;&#39;
        Query a workflow and return the raster result as a georeferenced xarray

        Parameters
        ----------
        bbox : A bounding box for the query
        timeout : HTTP request timeout in seconds
        force_no_data_value: If not None, use this value as no data value for the requested raster data. \
            Otherwise, use the Geo Engine will produce masked rasters.
        &#39;&#39;&#39;

        with self.__get_wcs_tiff_as_memory_file(
            bbox,
            timeout,
            force_no_data_value
        ) as memfile, memfile.open() as dataset:
            data_array = rioxarray.open_rasterio(dataset)

            # helping mypy with inference
            assert isinstance(data_array, xr.DataArray)

            rio: xr.DataArray = data_array.rio
            rio.update_attrs({
                &#39;crs&#39;: rio.crs,
                &#39;res&#39;: rio.resolution(),
                &#39;transform&#39;: rio.transform(),
            }, inplace=True)

            # TODO: add time information to dataset
            return data_array.load()

    # pylint: disable=too-many-arguments
    def download_raster(
        self,
        bbox: QueryRectangle,
        file_path: str,
        timeout=3600,
        file_format: str = &#39;image/tiff&#39;,
        force_no_data_value: Optional[float] = None
    ) -&gt; None:
        &#39;&#39;&#39;
        Query a workflow and save the raster result as a file on disk

        Parameters
        ----------
        bbox : A bounding box for the query
        file_path : The path to the file to save the raster to
        timeout : HTTP request timeout in seconds
        file_format : The format of the returned raster
        force_no_data_value: If not None, use this value as no data value for the requested raster data. \
            Otherwise, use the Geo Engine will produce masked rasters.
        &#39;&#39;&#39;

        response = self.__request_wcs(bbox, timeout, file_format, force_no_data_value)

        with open(file_path, &#39;wb&#39;) as file:
            file.write(response.read())

    def get_provenance(self, timeout: int = 60) -&gt; List[ProvenanceEntry]:
        &#39;&#39;&#39;
        Query the provenance of the workflow
        &#39;&#39;&#39;

        session = get_session()

        provenance_url = f&#39;{session.server_url}/workflow/{self.__workflow_id}/provenance&#39;

        response = req.get(provenance_url, headers=session.auth_header, timeout=timeout).json()

        return [ProvenanceEntry.from_response(item) for item in response]

    def metadata_zip(self, path: Union[PathLike, BytesIO], timeout: int = 60) -&gt; None:
        &#39;&#39;&#39;
        Query workflow metadata and citations and stores it as zip file to `path`
        &#39;&#39;&#39;

        session = get_session()

        provenance_url = f&#39;{session.server_url}/workflow/{self.__workflow_id}/allMetadata/zip&#39;

        response = req.get(provenance_url, headers=session.auth_header, timeout=timeout).content

        if isinstance(path, BytesIO):
            path.write(response)
        else:
            with open(path, &#39;wb&#39;) as file:
                file.write(response)

    def save_as_dataset(
            self,
            query_rectangle: api.RasterQueryRectangle,
            name: str,
            description: str = &#39;&#39;,
            timeout: int = 3600) -&gt; Task:
        &#39;&#39;&#39;Init task to store the workflow result as a layer&#39;&#39;&#39;

        # Currently, it only works for raster results
        if not self.__result_descriptor.is_raster_result():
            raise MethodNotCalledOnRasterException()

        session = get_session()

        request_body = {
            &#39;name&#39;: name,
            &#39;description&#39;: description,
            &#39;query&#39;: query_rectangle,
        }

        response = req.post(
            url=f&#39;{session.server_url}/datasetFromWorkflow/{self.__workflow_id}&#39;,
            json=request_body,
            headers=session.auth_header,
            timeout=timeout
        )

        check_response_for_error(response)

        return Task(TaskId.from_response(response.json()))

    async def raster_stream(
            self,
            query_rectangle: QueryRectangle,
            clip_to_query_rectangle: bool = False,
            open_timeout: int = 60) -&gt; AsyncIterator[xr.DataArray]:
        &#39;&#39;&#39;Stream the workflow result as series of xarrays&#39;&#39;&#39;

        def read_arrow_ipc(arrow_ipc: bytes) -&gt; pa.RecordBatch:
            reader = pa.ipc.open_file(arrow_ipc)
            # We know from the backend that there is only one record batch
            record_batch = reader.get_record_batch(0)
            return record_batch

        def create_xarray(record_batch: pa.RecordBatch) -&gt; xr.DataArray:
            metadata = record_batch.schema.metadata
            geo_transform: GeoTransform = GeoTransform.from_response(json.loads(metadata[b&#39;geoTransform&#39;]))
            x_size = int(metadata[b&#39;xSize&#39;])
            y_size = int(metadata[b&#39;ySize&#39;])
            spatial_reference = metadata[b&#39;spatialReference&#39;].decode(&#39;utf-8&#39;)
            # We know from the backend that there is only one array a.k.a. one column
            arrow_array = record_batch.column(0)

            time = TimeInterval.from_response(json.loads(metadata[b&#39;time&#39;]))

            array = xr.DataArray(
                arrow_array.to_numpy(
                    zero_copy_only=False,  # cannot zero-copy as soon as we have nodata values
                ).reshape(x_size, y_size),
                dims=[&#34;y&#34;, &#34;x&#34;],
                coords={
                    &#39;x&#39;: np.arange(
                        start=geo_transform.x_min + geo_transform.x_half_pixel_size,
                        step=geo_transform.x_pixel_size,
                        stop=geo_transform.x_max(x_size),
                    ),
                    &#39;y&#39;: np.arange(
                        start=geo_transform.y_max + geo_transform.y_half_pixel_size,
                        step=geo_transform.y_pixel_size,
                        stop=geo_transform.y_min(y_size),
                    ),
                    &#39;time&#39;: time.start,  # TODO: incorporate time end?
                },
            )

            array.rio.write_crs(spatial_reference, inplace=True)

            return array

        def process_bytes(tile_bytes: Optional[bytes]) -&gt; Optional[xr.DataArray]:
            if tile_bytes is None:
                return None

            # process the received data
            record_batch = read_arrow_ipc(tile_bytes)
            tile = create_xarray(record_batch)

            return tile

        # Currently, it only works for raster results
        if not self.__result_descriptor.is_raster_result():
            raise MethodNotCalledOnRasterException()

        session = get_session()

        url = req.Request(
            &#39;GET&#39;,
            url=f&#39;{session.server_url}/workflow/{self.__workflow_id}/rasterStream&#39;,
            params={
                &#39;resultType&#39;: &#39;arrow&#39;,
                &#39;spatialBounds&#39;: query_rectangle.bbox_str,
                &#39;timeInterval&#39;: query_rectangle.time_str,
                &#39;spatialResolution&#39;: str(query_rectangle.spatial_resolution),
            },
        ).prepare().url

        if url is None:
            raise InputException(&#39;Invalid websocket url&#39;)

        # for the websockets library, it is necessary that the url starts with `ws://``
        [_, url_part] = url.split(&#39;://&#39;, maxsplit=1)

        async with websockets.client.connect(
            uri=f&#39;ws://{url_part}&#39;,
            extra_headers=session.auth_header,
            open_timeout=open_timeout,
        ) as websocket:

            tile_bytes: Optional[bytes] = None

            while websocket.open:
                async def read_new_bytes() -&gt; Optional[bytes]:
                    # already send the next request to speed up the process
                    try:
                        await websocket.send(&#34;NEXT&#34;)
                    except websockets.exceptions.ConnectionClosed:
                        # the websocket connection is already closed, we cannot read anymore
                        return None

                    try:
                        data: Union[str, bytes] = await websocket.recv()

                        if isinstance(data, str):
                            # the server sent an error message
                            raise GeoEngineException({&#39;error&#39;: data})

                        return data
                    except websockets.exceptions.ConnectionClosedOK:
                        # the websocket connection closed gracefully, so we stop reading
                        return None

                (tile_bytes, tile) = await asyncio.gather(
                    read_new_bytes(),
                    # asyncio.to_thread(process_bytes, tile_bytes), # TODO: use this when min Python version is 3.9
                    backports.to_thread(process_bytes, tile_bytes),
                )

                if tile is not None:
                    if clip_to_query_rectangle:
                        tile = tile.rio.clip_box(*query_rectangle.spatial_bounds.as_bbox_tuple())
                        tile = cast(xr.DataArray, tile)

                    yield tile

            # process the last tile
            tile = process_bytes(tile_bytes)

            if tile is not None:
                if clip_to_query_rectangle:
                    tile = tile.rio.clip_box(*query_rectangle.spatial_bounds.as_bbox_tuple())
                    tile = cast(xr.DataArray, tile)

                yield tile

    async def raster_stream_into_xarray(
            self,
            query_rectangle: QueryRectangle,
            clip_to_query_rectangle: bool = False,
            open_timeout: int = 60) -&gt; xr.DataArray:
        &#39;&#39;&#39;
        Stream the workflow result into memory and output a single xarray.

        NOTE: You can run out of memory if the query rectangle is too large.
        &#39;&#39;&#39;

        tile_stream = self.raster_stream(
            query_rectangle,
            clip_to_query_rectangle=clip_to_query_rectangle,
            open_timeout=open_timeout
        )

        timesteps: List[xr.DataArray] = []

        async def read_tiles(
            remainder_tile: Optional[xr.DataArray]
        ) -&gt; tuple[List[xr.DataArray], Optional[xr.DataArray]]:
            last_timestep: Optional[np.datetime64] = None
            tiles = []

            if remainder_tile is not None:
                last_timestep = remainder_tile.time.values
                tiles.append(remainder_tile)

            async for tile in tile_stream:
                timestep: np.datetime64 = tile.time.values
                if last_timestep is None:
                    last_timestep = timestep
                elif last_timestep != timestep:
                    return tiles, tile

                tiles.append(tile)

            # this seems to be the last time step, so just return tiles
            return tiles, None

        def merge_tiles(tiles: List[xr.DataArray]) -&gt; Optional[xr.DataArray]:
            if len(tiles) == 0:
                return None

            combined_tiles = xr.combine_by_coords(tiles)

            if isinstance(combined_tiles, xr.Dataset):
                raise TypeException(&#39;Internal error: Merging data arrays should result in a data array.&#39;)

            return combined_tiles

        (tiles, remainder_tile) = await read_tiles(None)

        while len(tiles):
            ((new_tiles, new_remainder_tile), new_timestep) = await asyncio.gather(
                read_tiles(remainder_tile),
                backports.to_thread(merge_tiles, tiles)
                # asyncio.to_thread(merge_tiles, tiles), # TODO: use this when min Python version is 3.9
            )

            tiles = new_tiles
            remainder_tile = new_remainder_tile

            if new_timestep is not None:
                timesteps.append(new_timestep)

        output: xr.DataArray = cast(
            xr.DataArray,
            # await asyncio.to_thread( # TODO: use this when min Python version is 3.9
            await backports.to_thread(
                xr.concat,
                # TODO: This is a typings error, since the method accepts also a `xr.DataArray` and returns one
                cast(List[xr.Dataset], timesteps),
                dim=&#39;time&#39;
            )
        )

        return output

    async def vector_stream(
            self,
            query_rectangle: QueryRectangle,
            time_start_column: str = &#39;time_start&#39;,
            time_end_column: str = &#39;time_end&#39;,
            open_timeout: int = 60) -&gt; AsyncIterator[gpd.GeoDataFrame]:
        &#39;&#39;&#39;Stream the workflow result as series of `GeoDataFrame`s&#39;&#39;&#39;

        def read_arrow_ipc(arrow_ipc: bytes) -&gt; pa.RecordBatch:
            reader = pa.ipc.open_file(arrow_ipc)
            # We know from the backend that there is only one record batch
            record_batch = reader.get_record_batch(0)
            return record_batch

        def create_geo_data_frame(record_batch: pa.RecordBatch,
                                  time_start_column: str,
                                  time_end_column: str) -&gt; gpd.GeoDataFrame:
            metadata = record_batch.schema.metadata
            spatial_reference = metadata[b&#39;spatialReference&#39;].decode(&#39;utf-8&#39;)

            data_frame = record_batch.to_pandas()

            geometry = gpd.GeoSeries.from_wkt(data_frame[api.GEOMETRY_COLUMN_NAME])
            del data_frame[api.GEOMETRY_COLUMN_NAME]  # delete the duplicated column

            geo_data_frame = gpd.GeoDataFrame(
                data_frame,
                geometry=geometry,
                crs=spatial_reference,
            )

            # split time column
            geo_data_frame[[time_start_column, time_end_column]] = geo_data_frame[api.TIME_COLUMN_NAME].tolist()
            del geo_data_frame[api.TIME_COLUMN_NAME]  # delete the duplicated column

            # parse time columns
            for time_column in [time_start_column, time_end_column]:
                geo_data_frame[time_column] = pd.to_datetime(
                    geo_data_frame[time_column],
                    utc=True,
                    unit=&#39;ms&#39;,
                    # TODO: solve time conversion problem from Geo Engine to Python for large (+/-) time instances
                    errors=&#39;coerce&#39;,
                )

            return geo_data_frame

        def process_bytes(batch_bytes: Optional[bytes]) -&gt; Optional[gpd.GeoDataFrame]:
            if batch_bytes is None:
                return None

            # process the received data
            record_batch = read_arrow_ipc(batch_bytes)
            tile = create_geo_data_frame(
                record_batch,
                time_start_column=time_start_column,
                time_end_column=time_end_column,
            )

            return tile

        # Currently, it only works for raster results
        if not self.__result_descriptor.is_vector_result():
            raise MethodNotCalledOnVectorException()

        session = get_session()

        url = req.Request(
            &#39;GET&#39;,
            url=f&#39;{session.server_url}/workflow/{self.__workflow_id}/vectorStream&#39;,
            params={
                &#39;resultType&#39;: &#39;arrow&#39;,
                &#39;spatialBounds&#39;: query_rectangle.bbox_str,
                &#39;timeInterval&#39;: query_rectangle.time_str,
                &#39;spatialResolution&#39;: str(query_rectangle.spatial_resolution),
            },
        ).prepare().url

        if url is None:
            raise InputException(&#39;Invalid websocket url&#39;)

        # for the websockets library, it is necessary that the url starts with `ws://``
        [_, url_part] = url.split(&#39;://&#39;, maxsplit=1)

        async with websockets.client.connect(
            uri=f&#39;ws://{url_part}&#39;,
            extra_headers=session.auth_header,
            open_timeout=open_timeout,
            max_size=None,  # allow arbitrary large messages, since it is capped by the server&#39;s chunk size
        ) as websocket:

            batch_bytes: Optional[bytes] = None

            while websocket.open:
                async def read_new_bytes() -&gt; Optional[bytes]:
                    # already send the next request to speed up the process
                    try:
                        await websocket.send(&#34;NEXT&#34;)
                    except websockets.exceptions.ConnectionClosed:
                        # the websocket connection is already closed, we cannot read anymore
                        return None

                    try:
                        data: Union[str, bytes] = await websocket.recv()

                        if isinstance(data, str):
                            # the server sent an error message
                            raise GeoEngineException({&#39;error&#39;: data})

                        return data
                    except websockets.exceptions.ConnectionClosedOK:
                        # the websocket connection closed gracefully, so we stop reading
                        return None

                (batch_bytes, batch) = await asyncio.gather(
                    read_new_bytes(),
                    # asyncio.to_thread(process_bytes, batch_bytes), # TODO: use this when min Python version is 3.9
                    backports.to_thread(process_bytes, batch_bytes),
                )

                if batch is not None:
                    yield batch

            # process the last tile
            batch = process_bytes(batch_bytes)

            if batch is not None:
                yield batch

    async def vector_stream_into_geopandas(
            self,
            query_rectangle: QueryRectangle,
            time_start_column: str = &#39;time_start&#39;,
            time_end_column: str = &#39;time_end&#39;,
            open_timeout: int = 60) -&gt; gpd.GeoDataFrame:
        &#39;&#39;&#39;
        Stream the workflow result into memory and output a single geo data frame.

        NOTE: You can run out of memory if the query rectangle is too large.
        &#39;&#39;&#39;

        chunk_stream = self.vector_stream(
            query_rectangle,
            time_start_column=time_start_column,
            time_end_column=time_end_column,
            open_timeout=open_timeout,
        )

        data_frame: Optional[gpd.GeoDataFrame] = None
        chunk: Optional[gpd.GeoDataFrame] = None

        async def read_dataframe() -&gt; Optional[gpd.GeoDataFrame]:
            try:
                return await chunk_stream.__anext__()
            except StopAsyncIteration:
                return None

        def merge_dataframes(
            df_a: Optional[gpd.GeoDataFrame],
            df_b: Optional[gpd.GeoDataFrame]
        ) -&gt; Optional[gpd.GeoDataFrame]:
            if df_a is None:
                return df_b

            if df_b is None:
                return df_a

            return pd.concat([df_a, df_b], ignore_index=True)

        while True:
            (chunk, data_frame) = await asyncio.gather(
                read_dataframe(),
                backports.to_thread(merge_dataframes, data_frame, chunk),
                # TODO: use this when min Python version is 3.9
                # asyncio.to_thread(merge_dataframes, data_frame, chunk),
            )

            # we can stop when the chunk stream is exhausted
            if chunk is None:
                break

        return data_frame


def register_workflow(workflow: Dict[str, Any], timeout: int = 60) -&gt; Workflow:
    &#39;&#39;&#39;
    Register a workflow in Geo Engine and receive a `WorkflowId`
    &#39;&#39;&#39;

    session = get_session()

    workflow_response = req.post(
        f&#39;{session.server_url}/workflow&#39;,
        json=workflow,
        headers=session.auth_header,
        timeout=timeout
    ).json()

    return Workflow(WorkflowId.from_response(workflow_response))


def workflow_by_id(workflow_id: UUID) -&gt; Workflow:
    &#39;&#39;&#39;
    Create a workflow object from a workflow id
    &#39;&#39;&#39;

    # TODO: check that workflow exists

    return Workflow(WorkflowId(workflow_id))


def get_quota(user_id: Optional[UUID] = None, timeout: int = 60) -&gt; api.Quota:
    &#39;&#39;&#39;
    Gets a user&#39;s quota. Only admins can get other users&#39; quota.
    &#39;&#39;&#39;

    session = get_session()

    url = f&#39;{session.server_url}/quota&#39;

    if user_id is not None:
        url = f&#39;{session.server_url}/quotas/{user_id}&#39;

    quota_response = req.get(
        url,
        headers=session.auth_header,
        timeout=timeout
    ).json()

    return api.Quota({
        &#34;available&#34;: quota_response[&#34;available&#34;],
        &#34;used&#34;: quota_response[&#34;used&#34;]
    })


def update_quota(user_id: UUID, new_available_quota: int, timeout: int = 60) -&gt; None:
    &#39;&#39;&#39;
    Update a user&#39;s quota. Only admins can perform this operation.
    &#39;&#39;&#39;

    session = get_session()

    req.post(
        f&#39;{session.server_url}/quotas/{user_id}&#39;,
        headers=session.auth_header,
        json=api.UpdateQuota({
            &#39;available&#39;: new_available_quota
        }),
        timeout=timeout
    )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="geoengine.workflow.get_quota"><code class="name flex">
<span>def <span class="ident">get_quota</span></span>(<span>user_id: Optional[UUID] = None, timeout: int = 60) ‑> <a title="geoengine.api.Quota" href="api.html#geoengine.api.Quota">Quota</a></span>
</code></dt>
<dd>
<div class="desc"><p>Gets a user's quota. Only admins can get other users' quota.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_quota(user_id: Optional[UUID] = None, timeout: int = 60) -&gt; api.Quota:
    &#39;&#39;&#39;
    Gets a user&#39;s quota. Only admins can get other users&#39; quota.
    &#39;&#39;&#39;

    session = get_session()

    url = f&#39;{session.server_url}/quota&#39;

    if user_id is not None:
        url = f&#39;{session.server_url}/quotas/{user_id}&#39;

    quota_response = req.get(
        url,
        headers=session.auth_header,
        timeout=timeout
    ).json()

    return api.Quota({
        &#34;available&#34;: quota_response[&#34;available&#34;],
        &#34;used&#34;: quota_response[&#34;used&#34;]
    })</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.register_workflow"><code class="name flex">
<span>def <span class="ident">register_workflow</span></span>(<span>workflow: Dict[str, Any], timeout: int = 60) ‑> <a title="geoengine.workflow.Workflow" href="#geoengine.workflow.Workflow">Workflow</a></span>
</code></dt>
<dd>
<div class="desc"><p>Register a workflow in Geo Engine and receive a <code><a title="geoengine.workflow.WorkflowId" href="#geoengine.workflow.WorkflowId">WorkflowId</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def register_workflow(workflow: Dict[str, Any], timeout: int = 60) -&gt; Workflow:
    &#39;&#39;&#39;
    Register a workflow in Geo Engine and receive a `WorkflowId`
    &#39;&#39;&#39;

    session = get_session()

    workflow_response = req.post(
        f&#39;{session.server_url}/workflow&#39;,
        json=workflow,
        headers=session.auth_header,
        timeout=timeout
    ).json()

    return Workflow(WorkflowId.from_response(workflow_response))</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.update_quota"><code class="name flex">
<span>def <span class="ident">update_quota</span></span>(<span>user_id: UUID, new_available_quota: int, timeout: int = 60) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Update a user's quota. Only admins can perform this operation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_quota(user_id: UUID, new_available_quota: int, timeout: int = 60) -&gt; None:
    &#39;&#39;&#39;
    Update a user&#39;s quota. Only admins can perform this operation.
    &#39;&#39;&#39;

    session = get_session()

    req.post(
        f&#39;{session.server_url}/quotas/{user_id}&#39;,
        headers=session.auth_header,
        json=api.UpdateQuota({
            &#39;available&#39;: new_available_quota
        }),
        timeout=timeout
    )</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.workflow_by_id"><code class="name flex">
<span>def <span class="ident">workflow_by_id</span></span>(<span>workflow_id: UUID) ‑> <a title="geoengine.workflow.Workflow" href="#geoengine.workflow.Workflow">Workflow</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create a workflow object from a workflow id</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def workflow_by_id(workflow_id: UUID) -&gt; Workflow:
    &#39;&#39;&#39;
    Create a workflow object from a workflow id
    &#39;&#39;&#39;

    # TODO: check that workflow exists

    return Workflow(WorkflowId(workflow_id))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="geoengine.workflow.Axis"><code class="flex name class">
<span>class <span class="ident">Axis</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="geoengine.workflow.Axis.title"><code class="name">var <span class="ident">title</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="geoengine.workflow.Bin"><code class="flex name class">
<span>class <span class="ident">Bin</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="geoengine.workflow.Bin.binned"><code class="name">var <span class="ident">binned</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="geoengine.workflow.Bin.step"><code class="name">var <span class="ident">step</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="geoengine.workflow.DatasetIds"><code class="flex name class">
<span>class <span class="ident">DatasetIds</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="geoengine.workflow.DatasetIds.dataset"><code class="name">var <span class="ident">dataset</span> : uuid.UUID</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="geoengine.workflow.DatasetIds.upload"><code class="name">var <span class="ident">upload</span> : uuid.UUID</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="geoengine.workflow.Encoding"><code class="flex name class">
<span>class <span class="ident">Encoding</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="geoengine.workflow.Encoding.x"><code class="name">var <span class="ident">x</span> : <a title="geoengine.workflow.X" href="#geoengine.workflow.X">X</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="geoengine.workflow.Encoding.x2"><code class="name">var <span class="ident">x2</span> : <a title="geoengine.workflow.X2" href="#geoengine.workflow.X2">X2</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="geoengine.workflow.Encoding.y"><code class="name">var <span class="ident">y</span> : <a title="geoengine.workflow.Y" href="#geoengine.workflow.Y">Y</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="geoengine.workflow.Field"><code class="flex name class">
<span>class <span class="ident">Field</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="geoengine.workflow.Field.field"><code class="name">var <span class="ident">field</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="geoengine.workflow.Values"><code class="flex name class">
<span>class <span class="ident">Values</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="geoengine.workflow.Values.Frequency"><code class="name">var <span class="ident">Frequency</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="geoengine.workflow.Values.binEnd"><code class="name">var <span class="ident">binEnd</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="geoengine.workflow.Values.binStart"><code class="name">var <span class="ident">binStart</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="geoengine.workflow.VegaSpec"><code class="flex name class">
<span>class <span class="ident">VegaSpec</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="geoengine.workflow.VegaSpec.$schema"><code class="name">var <span class="ident">$schema</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="geoengine.workflow.VegaSpec.data"><code class="name">var <span class="ident">data</span> : List[<a title="geoengine.workflow.Values" href="#geoengine.workflow.Values">Values</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="geoengine.workflow.VegaSpec.encoding"><code class="name">var <span class="ident">encoding</span> : <a title="geoengine.workflow.Encoding" href="#geoengine.workflow.Encoding">Encoding</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="geoengine.workflow.VegaSpec.mark"><code class="name">var <span class="ident">mark</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="geoengine.workflow.Workflow"><code class="flex name class">
<span>class <span class="ident">Workflow</span></span>
<span>(</span><span>workflow_id: <a title="geoengine.workflow.WorkflowId" href="#geoengine.workflow.WorkflowId">WorkflowId</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Holds a workflow id and allows querying data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Workflow:
    &#39;&#39;&#39;
    Holds a workflow id and allows querying data
    &#39;&#39;&#39;

    __workflow_id: WorkflowId
    __result_descriptor: ResultDescriptor

    def __init__(self, workflow_id: WorkflowId) -&gt; None:
        self.__workflow_id = workflow_id
        self.__result_descriptor = self.__query_result_descriptor()

    def __str__(self) -&gt; str:
        return str(self.__workflow_id)

    def __repr__(self) -&gt; str:
        return repr(self.__workflow_id)

    def __query_result_descriptor(self, timeout: int = 60) -&gt; ResultDescriptor:
        &#39;&#39;&#39;
        Query the metadata of the workflow result
        &#39;&#39;&#39;

        session = get_session()

        response = req.get(
            f&#39;{session.server_url}/workflow/{self.__workflow_id}/metadata&#39;,
            headers=session.auth_header,
            timeout=timeout
        ).json()

        debug(response)

        return ResultDescriptor.from_response(response)

    def get_result_descriptor(self) -&gt; ResultDescriptor:
        &#39;&#39;&#39;
        Return the metadata of the workflow result
        &#39;&#39;&#39;

        return self.__result_descriptor

    def workflow_definition(self, timeout: int = 60) -&gt; Dict[str, Any]:
        &#39;&#39;&#39;Return the workflow definition for this workflow&#39;&#39;&#39;

        session = get_session()

        response = req.get(
            f&#39;{session.server_url}/workflow/{self.__workflow_id}&#39;,
            headers=session.auth_header,
            timeout=timeout
        ).json()

        return response

    def __get_wfs_url(self, bbox: QueryRectangle) -&gt; str:
        &#39;&#39;&#39;Build a WFS url from a workflow and a `QueryRectangle`&#39;&#39;&#39;

        session = get_session()

        params = {
            &#39;service&#39;: &#39;WFS&#39;,
            &#39;version&#39;: &#34;2.0.0&#34;,
            &#39;request&#39;: &#39;GetFeature&#39;,
            &#39;outputFormat&#39;: &#39;application/json&#39;,
            &#39;typeNames&#39;: f&#39;{self.__workflow_id}&#39;,
            &#39;bbox&#39;: bbox.bbox_str,
            &#39;time&#39;: bbox.time_str,
            &#39;srsName&#39;: bbox.srs,
            &#39;queryResolution&#39;: str(bbox.spatial_resolution)
        }

        wfs_url = req.Request(
            &#39;GET&#39;, url=f&#39;{session.server_url}/wfs/{self.__workflow_id}&#39;, params=params).prepare().url

        debug(f&#39;WFS URL:\n{wfs_url}&#39;)

        if not wfs_url:
            raise InvalidUrlException(&#39;Failed to build WFS URL for workflow {self.__workflow_id}.&#39;)
        return wfs_url

    def get_wfs_get_feature_curl(self, bbox: QueryRectangle) -&gt; str:
        &#39;&#39;&#39;Return the WFS url for a workflow and a `QueryRectangle` as a cURL command&#39;&#39;&#39;

        if not self.__result_descriptor.is_vector_result():
            raise MethodNotCalledOnVectorException()

        wfs_request = req.Request(
            &#39;GET&#39;,
            url=self.__get_wfs_url(bbox),
            headers=get_session().auth_header
        ).prepare()

        command = &#34;curl -X {method} -H {headers} &#39;{uri}&#39;&#34;
        headers_list = [f&#39;&#34;{k}: {v}&#34;&#39; for k, v in wfs_request.headers.items()]
        headers = &#34; -H &#34;.join(headers_list)
        return command.format(method=wfs_request.method, headers=headers, uri=wfs_request.url)

    def get_dataframe(self, bbox: QueryRectangle, timeout: int = 3600) -&gt; gpd.GeoDataFrame:
        &#39;&#39;&#39;
        Query a workflow and return the WFS result as a GeoPandas `GeoDataFrame`
        &#39;&#39;&#39;

        if not self.__result_descriptor.is_vector_result():
            raise MethodNotCalledOnVectorException()

        session = get_session()

        wfs_url = self.__get_wfs_url(bbox)

        data_response = req.get(wfs_url, headers=session.auth_header, timeout=timeout)

        check_response_for_error(data_response)

        data = data_response.json()

        def geo_json_with_time_to_geopandas(geo_json):
            &#39;&#39;&#39;
            GeoJson has no standard for time, so we parse the when field
            separately and attach it to the data frame as columns `start`
            and `end`.
            &#39;&#39;&#39;

            data = gpd.GeoDataFrame.from_features(geo_json)
            data = data.set_crs(bbox.srs, allow_override=True)

            start = [f[&#39;when&#39;][&#39;start&#39;] for f in geo_json[&#39;features&#39;]]
            end = [f[&#39;when&#39;][&#39;end&#39;] for f in geo_json[&#39;features&#39;]]

            # TODO: find a good way to infer BoT/EoT

            data[&#39;start&#39;] = gpd.pd.to_datetime(start, errors=&#39;coerce&#39;)
            data[&#39;end&#39;] = gpd.pd.to_datetime(end, errors=&#39;coerce&#39;)

            return data

        return geo_json_with_time_to_geopandas(data)

    def wms_get_map_as_image(self, bbox: QueryRectangle, colorizer: Colorizer) -&gt; Image:
        &#39;&#39;&#39;Return the result of a WMS request as a PIL Image&#39;&#39;&#39;

        wms_request = self.__wms_get_map_request(bbox, colorizer)
        response = req.Session().send(wms_request)

        check_response_for_error(response)

        return Image.open(BytesIO(response.content))

    def __wms_get_map_request(self,
                              bbox: QueryRectangle,
                              colorizer: Colorizer) -&gt; req.PreparedRequest:
        &#39;&#39;&#39;Return the WMS url for a workflow and a given `QueryRectangle`&#39;&#39;&#39;

        if not self.__result_descriptor.is_raster_result():
            raise MethodNotCalledOnRasterException()

        session = get_session()

        width = int((bbox.spatial_bounds.xmax - bbox.spatial_bounds.xmin) / bbox.spatial_resolution.x_resolution)
        height = int((bbox.spatial_bounds.ymax - bbox.spatial_bounds.ymin) / bbox.spatial_resolution.y_resolution)

        colorizer_colorizer_str = &#39;custom:&#39; + colorizer.to_json()

        params = {
            &#39;service&#39;: &#39;WMS&#39;,
            &#39;version&#39;: &#39;1.3.0&#39;,
            &#39;request&#39;: &#34;GetMap&#34;,
            &#39;layers&#39;: str(self),
            &#39;time&#39;: bbox.time_str,
            &#39;crs&#39;: bbox.srs,
            &#39;bbox&#39;: bbox.bbox_ogc_str,
            &#39;width&#39;: width,
            &#39;height&#39;: height,
            &#39;format&#39;: &#39;image/png&#39;,
            &#39;styles&#39;: colorizer_colorizer_str,
        }

        return req.Request(
            &#39;GET&#39;,
            url=f&#39;{session.server_url}/wms/{str(self)}&#39;,
            params=params,
            headers=session.auth_header
        ).prepare()

    def wms_get_map_curl(self, bbox: QueryRectangle, colorizer: Colorizer) -&gt; str:
        &#39;&#39;&#39;Return the WMS curl command for a workflow and a given `QueryRectangle`&#39;&#39;&#39;

        wms_request = self.__wms_get_map_request(bbox, colorizer)

        command = &#34;curl -X {method} -H {headers} &#39;{uri}&#39;&#34;
        headers_list = [f&#39;&#34;{k}: {v}&#34;&#39; for k, v in wms_request.headers.items()]
        headers = &#34; -H &#34;.join(headers_list)
        return command.format(method=wms_request.method, headers=headers, uri=wms_request.url)

    def plot_chart(self, bbox: QueryRectangle, timeout: int = 3600) -&gt; VegaLite:
        &#39;&#39;&#39;
        Query a workflow and return the plot chart result as a vega plot
        &#39;&#39;&#39;

        if not self.__result_descriptor.is_plot_result():
            raise MethodNotCalledOnPlotException()

        session = get_session()

        time = urllib.parse.quote(bbox.time_str)
        spatial_bounds = urllib.parse.quote(bbox.bbox_str)
        resolution = str(bbox.spatial_resolution)

        plot_url = f&#39;{session.server_url}/plot/{self}?bbox={spatial_bounds}&amp;crs={bbox.srs}&amp;time={time}&#39;\
            f&#39;&amp;spatialResolution={resolution}&#39;

        response = req.get(plot_url, headers=session.auth_header, timeout=timeout)

        check_response_for_error(response)

        response_json: JsonType = response.json()
        assert isinstance(response_json, Dict)

        vega_spec: VegaSpec = json.loads(response_json[&#39;data&#39;][&#39;vegaString&#39;])

        return VegaLite(vega_spec)

    def __request_wcs(
        self,
        bbox: QueryRectangle,
        timeout=3600,
        file_format: str = &#39;image/tiff&#39;,
        force_no_data_value: Optional[float] = None
    ) -&gt; ResponseWrapper:
        &#39;&#39;&#39;
        Query a workflow and return the coverage

        Parameters
        ----------
        bbox : A bounding box for the query
        timeout : HTTP request timeout in seconds
        file_format : The format of the returned raster
        force_no_data_value: If not None, use this value as no data value for the requested raster data. \
            Otherwise, use the Geo Engine will produce masked rasters.
        &#39;&#39;&#39;

        if not self.__result_descriptor.is_raster_result():
            raise MethodNotCalledOnRasterException()

        session = get_session()

        # TODO: properly build CRS string for bbox
        crs = f&#39;urn:ogc:def:crs:{bbox.srs.replace(&#34;:&#34;, &#34;::&#34;)}&#39;

        wcs_url = f&#39;{session.server_url}/wcs/{self.__workflow_id}&#39;
        wcs = WebCoverageService(
            wcs_url,
            version=&#39;1.1.1&#39;,
            auth=Authentication(auth_delegate=session.requests_bearer_auth()),
        )

        [resx, resy] = bbox.resolution_ogc

        no_data_value = &#34;&#34;
        if force_no_data_value is not None:
            no_data_value = str(float(force_no_data_value))

        return wcs.getCoverage(
            identifier=f&#39;{self.__workflow_id}&#39;,
            bbox=bbox.bbox_ogc,
            time=[urllib.parse.quote_plus(bbox.time_str)],
            format=file_format,
            crs=crs,
            resx=resx,
            resy=resy,
            timeout=timeout,
            nodatavalue=no_data_value,
        )

    def __get_wcs_tiff_as_memory_file(
        self,
        bbox: QueryRectangle,
        timeout=3600,
        force_no_data_value: Optional[float] = None
    ) -&gt; rasterio.io.MemoryFile:
        &#39;&#39;&#39;
        Query a workflow and return the raster result as a memory mapped GeoTiff

        Parameters
        ----------
        bbox : A bounding box for the query
        timeout : HTTP request timeout in seconds
        force_no_data_value: If not None, use this value as no data value for the requested raster data. \
            Otherwise, use the Geo Engine will produce masked rasters.
        &#39;&#39;&#39;

        response = self.__request_wcs(bbox, timeout, &#39;image/tiff&#39;, force_no_data_value).read()

        # response is checked via `raise_on_error` in `getCoverage` / `openUrl`

        memory_file = rasterio.io.MemoryFile(response)

        return memory_file

    def get_array(
        self,
        bbox: QueryRectangle,
        timeout=3600,
        force_no_data_value: Optional[float] = None
    ) -&gt; np.ndarray:
        &#39;&#39;&#39;
        Query a workflow and return the raster result as a numpy array

        Parameters
        ----------
        bbox : A bounding box for the query
        timeout : HTTP request timeout in seconds
        force_no_data_value: If not None, use this value as no data value for the requested raster data. \
            Otherwise, use the Geo Engine will produce masked rasters.
        &#39;&#39;&#39;

        with self.__get_wcs_tiff_as_memory_file(
            bbox,
            timeout,
            force_no_data_value
        ) as memfile, memfile.open() as dataset:
            array = dataset.read(1)

            return array

    def get_xarray(
        self,
        bbox: QueryRectangle,
        timeout=3600,
        force_no_data_value: Optional[float] = None
    ) -&gt; xr.DataArray:
        &#39;&#39;&#39;
        Query a workflow and return the raster result as a georeferenced xarray

        Parameters
        ----------
        bbox : A bounding box for the query
        timeout : HTTP request timeout in seconds
        force_no_data_value: If not None, use this value as no data value for the requested raster data. \
            Otherwise, use the Geo Engine will produce masked rasters.
        &#39;&#39;&#39;

        with self.__get_wcs_tiff_as_memory_file(
            bbox,
            timeout,
            force_no_data_value
        ) as memfile, memfile.open() as dataset:
            data_array = rioxarray.open_rasterio(dataset)

            # helping mypy with inference
            assert isinstance(data_array, xr.DataArray)

            rio: xr.DataArray = data_array.rio
            rio.update_attrs({
                &#39;crs&#39;: rio.crs,
                &#39;res&#39;: rio.resolution(),
                &#39;transform&#39;: rio.transform(),
            }, inplace=True)

            # TODO: add time information to dataset
            return data_array.load()

    # pylint: disable=too-many-arguments
    def download_raster(
        self,
        bbox: QueryRectangle,
        file_path: str,
        timeout=3600,
        file_format: str = &#39;image/tiff&#39;,
        force_no_data_value: Optional[float] = None
    ) -&gt; None:
        &#39;&#39;&#39;
        Query a workflow and save the raster result as a file on disk

        Parameters
        ----------
        bbox : A bounding box for the query
        file_path : The path to the file to save the raster to
        timeout : HTTP request timeout in seconds
        file_format : The format of the returned raster
        force_no_data_value: If not None, use this value as no data value for the requested raster data. \
            Otherwise, use the Geo Engine will produce masked rasters.
        &#39;&#39;&#39;

        response = self.__request_wcs(bbox, timeout, file_format, force_no_data_value)

        with open(file_path, &#39;wb&#39;) as file:
            file.write(response.read())

    def get_provenance(self, timeout: int = 60) -&gt; List[ProvenanceEntry]:
        &#39;&#39;&#39;
        Query the provenance of the workflow
        &#39;&#39;&#39;

        session = get_session()

        provenance_url = f&#39;{session.server_url}/workflow/{self.__workflow_id}/provenance&#39;

        response = req.get(provenance_url, headers=session.auth_header, timeout=timeout).json()

        return [ProvenanceEntry.from_response(item) for item in response]

    def metadata_zip(self, path: Union[PathLike, BytesIO], timeout: int = 60) -&gt; None:
        &#39;&#39;&#39;
        Query workflow metadata and citations and stores it as zip file to `path`
        &#39;&#39;&#39;

        session = get_session()

        provenance_url = f&#39;{session.server_url}/workflow/{self.__workflow_id}/allMetadata/zip&#39;

        response = req.get(provenance_url, headers=session.auth_header, timeout=timeout).content

        if isinstance(path, BytesIO):
            path.write(response)
        else:
            with open(path, &#39;wb&#39;) as file:
                file.write(response)

    def save_as_dataset(
            self,
            query_rectangle: api.RasterQueryRectangle,
            name: str,
            description: str = &#39;&#39;,
            timeout: int = 3600) -&gt; Task:
        &#39;&#39;&#39;Init task to store the workflow result as a layer&#39;&#39;&#39;

        # Currently, it only works for raster results
        if not self.__result_descriptor.is_raster_result():
            raise MethodNotCalledOnRasterException()

        session = get_session()

        request_body = {
            &#39;name&#39;: name,
            &#39;description&#39;: description,
            &#39;query&#39;: query_rectangle,
        }

        response = req.post(
            url=f&#39;{session.server_url}/datasetFromWorkflow/{self.__workflow_id}&#39;,
            json=request_body,
            headers=session.auth_header,
            timeout=timeout
        )

        check_response_for_error(response)

        return Task(TaskId.from_response(response.json()))

    async def raster_stream(
            self,
            query_rectangle: QueryRectangle,
            clip_to_query_rectangle: bool = False,
            open_timeout: int = 60) -&gt; AsyncIterator[xr.DataArray]:
        &#39;&#39;&#39;Stream the workflow result as series of xarrays&#39;&#39;&#39;

        def read_arrow_ipc(arrow_ipc: bytes) -&gt; pa.RecordBatch:
            reader = pa.ipc.open_file(arrow_ipc)
            # We know from the backend that there is only one record batch
            record_batch = reader.get_record_batch(0)
            return record_batch

        def create_xarray(record_batch: pa.RecordBatch) -&gt; xr.DataArray:
            metadata = record_batch.schema.metadata
            geo_transform: GeoTransform = GeoTransform.from_response(json.loads(metadata[b&#39;geoTransform&#39;]))
            x_size = int(metadata[b&#39;xSize&#39;])
            y_size = int(metadata[b&#39;ySize&#39;])
            spatial_reference = metadata[b&#39;spatialReference&#39;].decode(&#39;utf-8&#39;)
            # We know from the backend that there is only one array a.k.a. one column
            arrow_array = record_batch.column(0)

            time = TimeInterval.from_response(json.loads(metadata[b&#39;time&#39;]))

            array = xr.DataArray(
                arrow_array.to_numpy(
                    zero_copy_only=False,  # cannot zero-copy as soon as we have nodata values
                ).reshape(x_size, y_size),
                dims=[&#34;y&#34;, &#34;x&#34;],
                coords={
                    &#39;x&#39;: np.arange(
                        start=geo_transform.x_min + geo_transform.x_half_pixel_size,
                        step=geo_transform.x_pixel_size,
                        stop=geo_transform.x_max(x_size),
                    ),
                    &#39;y&#39;: np.arange(
                        start=geo_transform.y_max + geo_transform.y_half_pixel_size,
                        step=geo_transform.y_pixel_size,
                        stop=geo_transform.y_min(y_size),
                    ),
                    &#39;time&#39;: time.start,  # TODO: incorporate time end?
                },
            )

            array.rio.write_crs(spatial_reference, inplace=True)

            return array

        def process_bytes(tile_bytes: Optional[bytes]) -&gt; Optional[xr.DataArray]:
            if tile_bytes is None:
                return None

            # process the received data
            record_batch = read_arrow_ipc(tile_bytes)
            tile = create_xarray(record_batch)

            return tile

        # Currently, it only works for raster results
        if not self.__result_descriptor.is_raster_result():
            raise MethodNotCalledOnRasterException()

        session = get_session()

        url = req.Request(
            &#39;GET&#39;,
            url=f&#39;{session.server_url}/workflow/{self.__workflow_id}/rasterStream&#39;,
            params={
                &#39;resultType&#39;: &#39;arrow&#39;,
                &#39;spatialBounds&#39;: query_rectangle.bbox_str,
                &#39;timeInterval&#39;: query_rectangle.time_str,
                &#39;spatialResolution&#39;: str(query_rectangle.spatial_resolution),
            },
        ).prepare().url

        if url is None:
            raise InputException(&#39;Invalid websocket url&#39;)

        # for the websockets library, it is necessary that the url starts with `ws://``
        [_, url_part] = url.split(&#39;://&#39;, maxsplit=1)

        async with websockets.client.connect(
            uri=f&#39;ws://{url_part}&#39;,
            extra_headers=session.auth_header,
            open_timeout=open_timeout,
        ) as websocket:

            tile_bytes: Optional[bytes] = None

            while websocket.open:
                async def read_new_bytes() -&gt; Optional[bytes]:
                    # already send the next request to speed up the process
                    try:
                        await websocket.send(&#34;NEXT&#34;)
                    except websockets.exceptions.ConnectionClosed:
                        # the websocket connection is already closed, we cannot read anymore
                        return None

                    try:
                        data: Union[str, bytes] = await websocket.recv()

                        if isinstance(data, str):
                            # the server sent an error message
                            raise GeoEngineException({&#39;error&#39;: data})

                        return data
                    except websockets.exceptions.ConnectionClosedOK:
                        # the websocket connection closed gracefully, so we stop reading
                        return None

                (tile_bytes, tile) = await asyncio.gather(
                    read_new_bytes(),
                    # asyncio.to_thread(process_bytes, tile_bytes), # TODO: use this when min Python version is 3.9
                    backports.to_thread(process_bytes, tile_bytes),
                )

                if tile is not None:
                    if clip_to_query_rectangle:
                        tile = tile.rio.clip_box(*query_rectangle.spatial_bounds.as_bbox_tuple())
                        tile = cast(xr.DataArray, tile)

                    yield tile

            # process the last tile
            tile = process_bytes(tile_bytes)

            if tile is not None:
                if clip_to_query_rectangle:
                    tile = tile.rio.clip_box(*query_rectangle.spatial_bounds.as_bbox_tuple())
                    tile = cast(xr.DataArray, tile)

                yield tile

    async def raster_stream_into_xarray(
            self,
            query_rectangle: QueryRectangle,
            clip_to_query_rectangle: bool = False,
            open_timeout: int = 60) -&gt; xr.DataArray:
        &#39;&#39;&#39;
        Stream the workflow result into memory and output a single xarray.

        NOTE: You can run out of memory if the query rectangle is too large.
        &#39;&#39;&#39;

        tile_stream = self.raster_stream(
            query_rectangle,
            clip_to_query_rectangle=clip_to_query_rectangle,
            open_timeout=open_timeout
        )

        timesteps: List[xr.DataArray] = []

        async def read_tiles(
            remainder_tile: Optional[xr.DataArray]
        ) -&gt; tuple[List[xr.DataArray], Optional[xr.DataArray]]:
            last_timestep: Optional[np.datetime64] = None
            tiles = []

            if remainder_tile is not None:
                last_timestep = remainder_tile.time.values
                tiles.append(remainder_tile)

            async for tile in tile_stream:
                timestep: np.datetime64 = tile.time.values
                if last_timestep is None:
                    last_timestep = timestep
                elif last_timestep != timestep:
                    return tiles, tile

                tiles.append(tile)

            # this seems to be the last time step, so just return tiles
            return tiles, None

        def merge_tiles(tiles: List[xr.DataArray]) -&gt; Optional[xr.DataArray]:
            if len(tiles) == 0:
                return None

            combined_tiles = xr.combine_by_coords(tiles)

            if isinstance(combined_tiles, xr.Dataset):
                raise TypeException(&#39;Internal error: Merging data arrays should result in a data array.&#39;)

            return combined_tiles

        (tiles, remainder_tile) = await read_tiles(None)

        while len(tiles):
            ((new_tiles, new_remainder_tile), new_timestep) = await asyncio.gather(
                read_tiles(remainder_tile),
                backports.to_thread(merge_tiles, tiles)
                # asyncio.to_thread(merge_tiles, tiles), # TODO: use this when min Python version is 3.9
            )

            tiles = new_tiles
            remainder_tile = new_remainder_tile

            if new_timestep is not None:
                timesteps.append(new_timestep)

        output: xr.DataArray = cast(
            xr.DataArray,
            # await asyncio.to_thread( # TODO: use this when min Python version is 3.9
            await backports.to_thread(
                xr.concat,
                # TODO: This is a typings error, since the method accepts also a `xr.DataArray` and returns one
                cast(List[xr.Dataset], timesteps),
                dim=&#39;time&#39;
            )
        )

        return output

    async def vector_stream(
            self,
            query_rectangle: QueryRectangle,
            time_start_column: str = &#39;time_start&#39;,
            time_end_column: str = &#39;time_end&#39;,
            open_timeout: int = 60) -&gt; AsyncIterator[gpd.GeoDataFrame]:
        &#39;&#39;&#39;Stream the workflow result as series of `GeoDataFrame`s&#39;&#39;&#39;

        def read_arrow_ipc(arrow_ipc: bytes) -&gt; pa.RecordBatch:
            reader = pa.ipc.open_file(arrow_ipc)
            # We know from the backend that there is only one record batch
            record_batch = reader.get_record_batch(0)
            return record_batch

        def create_geo_data_frame(record_batch: pa.RecordBatch,
                                  time_start_column: str,
                                  time_end_column: str) -&gt; gpd.GeoDataFrame:
            metadata = record_batch.schema.metadata
            spatial_reference = metadata[b&#39;spatialReference&#39;].decode(&#39;utf-8&#39;)

            data_frame = record_batch.to_pandas()

            geometry = gpd.GeoSeries.from_wkt(data_frame[api.GEOMETRY_COLUMN_NAME])
            del data_frame[api.GEOMETRY_COLUMN_NAME]  # delete the duplicated column

            geo_data_frame = gpd.GeoDataFrame(
                data_frame,
                geometry=geometry,
                crs=spatial_reference,
            )

            # split time column
            geo_data_frame[[time_start_column, time_end_column]] = geo_data_frame[api.TIME_COLUMN_NAME].tolist()
            del geo_data_frame[api.TIME_COLUMN_NAME]  # delete the duplicated column

            # parse time columns
            for time_column in [time_start_column, time_end_column]:
                geo_data_frame[time_column] = pd.to_datetime(
                    geo_data_frame[time_column],
                    utc=True,
                    unit=&#39;ms&#39;,
                    # TODO: solve time conversion problem from Geo Engine to Python for large (+/-) time instances
                    errors=&#39;coerce&#39;,
                )

            return geo_data_frame

        def process_bytes(batch_bytes: Optional[bytes]) -&gt; Optional[gpd.GeoDataFrame]:
            if batch_bytes is None:
                return None

            # process the received data
            record_batch = read_arrow_ipc(batch_bytes)
            tile = create_geo_data_frame(
                record_batch,
                time_start_column=time_start_column,
                time_end_column=time_end_column,
            )

            return tile

        # Currently, it only works for raster results
        if not self.__result_descriptor.is_vector_result():
            raise MethodNotCalledOnVectorException()

        session = get_session()

        url = req.Request(
            &#39;GET&#39;,
            url=f&#39;{session.server_url}/workflow/{self.__workflow_id}/vectorStream&#39;,
            params={
                &#39;resultType&#39;: &#39;arrow&#39;,
                &#39;spatialBounds&#39;: query_rectangle.bbox_str,
                &#39;timeInterval&#39;: query_rectangle.time_str,
                &#39;spatialResolution&#39;: str(query_rectangle.spatial_resolution),
            },
        ).prepare().url

        if url is None:
            raise InputException(&#39;Invalid websocket url&#39;)

        # for the websockets library, it is necessary that the url starts with `ws://``
        [_, url_part] = url.split(&#39;://&#39;, maxsplit=1)

        async with websockets.client.connect(
            uri=f&#39;ws://{url_part}&#39;,
            extra_headers=session.auth_header,
            open_timeout=open_timeout,
            max_size=None,  # allow arbitrary large messages, since it is capped by the server&#39;s chunk size
        ) as websocket:

            batch_bytes: Optional[bytes] = None

            while websocket.open:
                async def read_new_bytes() -&gt; Optional[bytes]:
                    # already send the next request to speed up the process
                    try:
                        await websocket.send(&#34;NEXT&#34;)
                    except websockets.exceptions.ConnectionClosed:
                        # the websocket connection is already closed, we cannot read anymore
                        return None

                    try:
                        data: Union[str, bytes] = await websocket.recv()

                        if isinstance(data, str):
                            # the server sent an error message
                            raise GeoEngineException({&#39;error&#39;: data})

                        return data
                    except websockets.exceptions.ConnectionClosedOK:
                        # the websocket connection closed gracefully, so we stop reading
                        return None

                (batch_bytes, batch) = await asyncio.gather(
                    read_new_bytes(),
                    # asyncio.to_thread(process_bytes, batch_bytes), # TODO: use this when min Python version is 3.9
                    backports.to_thread(process_bytes, batch_bytes),
                )

                if batch is not None:
                    yield batch

            # process the last tile
            batch = process_bytes(batch_bytes)

            if batch is not None:
                yield batch

    async def vector_stream_into_geopandas(
            self,
            query_rectangle: QueryRectangle,
            time_start_column: str = &#39;time_start&#39;,
            time_end_column: str = &#39;time_end&#39;,
            open_timeout: int = 60) -&gt; gpd.GeoDataFrame:
        &#39;&#39;&#39;
        Stream the workflow result into memory and output a single geo data frame.

        NOTE: You can run out of memory if the query rectangle is too large.
        &#39;&#39;&#39;

        chunk_stream = self.vector_stream(
            query_rectangle,
            time_start_column=time_start_column,
            time_end_column=time_end_column,
            open_timeout=open_timeout,
        )

        data_frame: Optional[gpd.GeoDataFrame] = None
        chunk: Optional[gpd.GeoDataFrame] = None

        async def read_dataframe() -&gt; Optional[gpd.GeoDataFrame]:
            try:
                return await chunk_stream.__anext__()
            except StopAsyncIteration:
                return None

        def merge_dataframes(
            df_a: Optional[gpd.GeoDataFrame],
            df_b: Optional[gpd.GeoDataFrame]
        ) -&gt; Optional[gpd.GeoDataFrame]:
            if df_a is None:
                return df_b

            if df_b is None:
                return df_a

            return pd.concat([df_a, df_b], ignore_index=True)

        while True:
            (chunk, data_frame) = await asyncio.gather(
                read_dataframe(),
                backports.to_thread(merge_dataframes, data_frame, chunk),
                # TODO: use this when min Python version is 3.9
                # asyncio.to_thread(merge_dataframes, data_frame, chunk),
            )

            # we can stop when the chunk stream is exhausted
            if chunk is None:
                break

        return data_frame</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="geoengine.workflow.Workflow.download_raster"><code class="name flex">
<span>def <span class="ident">download_raster</span></span>(<span>self, bbox: QueryRectangle, file_path: str, timeout=3600, file_format: str = 'image/tiff', force_no_data_value: Optional[float] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Query a workflow and save the raster result as a file on disk</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>bbox</code></strong> :&ensp;<code>A bounding box for the query</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>file_path</code></strong> :&ensp;<code>The path to the file to save the raster to</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>timeout</code></strong> :&ensp;<code>HTTP request timeout in seconds</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>file_format</code></strong> :&ensp;<code>The format</code> of <code>the returned raster</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>force_no_data_value: If not None, use this value as no data value for the requested raster data.
Otherwise, use the Geo Engine will produce masked rasters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def download_raster(
    self,
    bbox: QueryRectangle,
    file_path: str,
    timeout=3600,
    file_format: str = &#39;image/tiff&#39;,
    force_no_data_value: Optional[float] = None
) -&gt; None:
    &#39;&#39;&#39;
    Query a workflow and save the raster result as a file on disk

    Parameters
    ----------
    bbox : A bounding box for the query
    file_path : The path to the file to save the raster to
    timeout : HTTP request timeout in seconds
    file_format : The format of the returned raster
    force_no_data_value: If not None, use this value as no data value for the requested raster data. \
        Otherwise, use the Geo Engine will produce masked rasters.
    &#39;&#39;&#39;

    response = self.__request_wcs(bbox, timeout, file_format, force_no_data_value)

    with open(file_path, &#39;wb&#39;) as file:
        file.write(response.read())</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.get_array"><code class="name flex">
<span>def <span class="ident">get_array</span></span>(<span>self, bbox: QueryRectangle, timeout=3600, force_no_data_value: Optional[float] = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Query a workflow and return the raster result as a numpy array</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>bbox</code></strong> :&ensp;<code>A bounding box for the query</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>timeout</code></strong> :&ensp;<code>HTTP request timeout in seconds</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>force_no_data_value: If not None, use this value as no data value for the requested raster data.
Otherwise, use the Geo Engine will produce masked rasters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_array(
    self,
    bbox: QueryRectangle,
    timeout=3600,
    force_no_data_value: Optional[float] = None
) -&gt; np.ndarray:
    &#39;&#39;&#39;
    Query a workflow and return the raster result as a numpy array

    Parameters
    ----------
    bbox : A bounding box for the query
    timeout : HTTP request timeout in seconds
    force_no_data_value: If not None, use this value as no data value for the requested raster data. \
        Otherwise, use the Geo Engine will produce masked rasters.
    &#39;&#39;&#39;

    with self.__get_wcs_tiff_as_memory_file(
        bbox,
        timeout,
        force_no_data_value
    ) as memfile, memfile.open() as dataset:
        array = dataset.read(1)

        return array</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.get_dataframe"><code class="name flex">
<span>def <span class="ident">get_dataframe</span></span>(<span>self, bbox: QueryRectangle, timeout: int = 3600) ‑> geopandas.geodataframe.GeoDataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Query a workflow and return the WFS result as a GeoPandas <code>GeoDataFrame</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dataframe(self, bbox: QueryRectangle, timeout: int = 3600) -&gt; gpd.GeoDataFrame:
    &#39;&#39;&#39;
    Query a workflow and return the WFS result as a GeoPandas `GeoDataFrame`
    &#39;&#39;&#39;

    if not self.__result_descriptor.is_vector_result():
        raise MethodNotCalledOnVectorException()

    session = get_session()

    wfs_url = self.__get_wfs_url(bbox)

    data_response = req.get(wfs_url, headers=session.auth_header, timeout=timeout)

    check_response_for_error(data_response)

    data = data_response.json()

    def geo_json_with_time_to_geopandas(geo_json):
        &#39;&#39;&#39;
        GeoJson has no standard for time, so we parse the when field
        separately and attach it to the data frame as columns `start`
        and `end`.
        &#39;&#39;&#39;

        data = gpd.GeoDataFrame.from_features(geo_json)
        data = data.set_crs(bbox.srs, allow_override=True)

        start = [f[&#39;when&#39;][&#39;start&#39;] for f in geo_json[&#39;features&#39;]]
        end = [f[&#39;when&#39;][&#39;end&#39;] for f in geo_json[&#39;features&#39;]]

        # TODO: find a good way to infer BoT/EoT

        data[&#39;start&#39;] = gpd.pd.to_datetime(start, errors=&#39;coerce&#39;)
        data[&#39;end&#39;] = gpd.pd.to_datetime(end, errors=&#39;coerce&#39;)

        return data

    return geo_json_with_time_to_geopandas(data)</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.get_provenance"><code class="name flex">
<span>def <span class="ident">get_provenance</span></span>(<span>self, timeout: int = 60) ‑> List[<a title="geoengine.types.ProvenanceEntry" href="types.html#geoengine.types.ProvenanceEntry">ProvenanceEntry</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Query the provenance of the workflow</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_provenance(self, timeout: int = 60) -&gt; List[ProvenanceEntry]:
    &#39;&#39;&#39;
    Query the provenance of the workflow
    &#39;&#39;&#39;

    session = get_session()

    provenance_url = f&#39;{session.server_url}/workflow/{self.__workflow_id}/provenance&#39;

    response = req.get(provenance_url, headers=session.auth_header, timeout=timeout).json()

    return [ProvenanceEntry.from_response(item) for item in response]</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.get_result_descriptor"><code class="name flex">
<span>def <span class="ident">get_result_descriptor</span></span>(<span>self) ‑> <a title="geoengine.types.ResultDescriptor" href="types.html#geoengine.types.ResultDescriptor">ResultDescriptor</a></span>
</code></dt>
<dd>
<div class="desc"><p>Return the metadata of the workflow result</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_result_descriptor(self) -&gt; ResultDescriptor:
    &#39;&#39;&#39;
    Return the metadata of the workflow result
    &#39;&#39;&#39;

    return self.__result_descriptor</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.get_wfs_get_feature_curl"><code class="name flex">
<span>def <span class="ident">get_wfs_get_feature_curl</span></span>(<span>self, bbox: QueryRectangle) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Return the WFS url for a workflow and a <code>QueryRectangle</code> as a cURL command</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_wfs_get_feature_curl(self, bbox: QueryRectangle) -&gt; str:
    &#39;&#39;&#39;Return the WFS url for a workflow and a `QueryRectangle` as a cURL command&#39;&#39;&#39;

    if not self.__result_descriptor.is_vector_result():
        raise MethodNotCalledOnVectorException()

    wfs_request = req.Request(
        &#39;GET&#39;,
        url=self.__get_wfs_url(bbox),
        headers=get_session().auth_header
    ).prepare()

    command = &#34;curl -X {method} -H {headers} &#39;{uri}&#39;&#34;
    headers_list = [f&#39;&#34;{k}: {v}&#34;&#39; for k, v in wfs_request.headers.items()]
    headers = &#34; -H &#34;.join(headers_list)
    return command.format(method=wfs_request.method, headers=headers, uri=wfs_request.url)</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.get_xarray"><code class="name flex">
<span>def <span class="ident">get_xarray</span></span>(<span>self, bbox: QueryRectangle, timeout=3600, force_no_data_value: Optional[float] = None) ‑> xarray.core.dataarray.DataArray</span>
</code></dt>
<dd>
<div class="desc"><p>Query a workflow and return the raster result as a georeferenced xarray</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>bbox</code></strong> :&ensp;<code>A bounding box for the query</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>timeout</code></strong> :&ensp;<code>HTTP request timeout in seconds</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>force_no_data_value: If not None, use this value as no data value for the requested raster data.
Otherwise, use the Geo Engine will produce masked rasters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_xarray(
    self,
    bbox: QueryRectangle,
    timeout=3600,
    force_no_data_value: Optional[float] = None
) -&gt; xr.DataArray:
    &#39;&#39;&#39;
    Query a workflow and return the raster result as a georeferenced xarray

    Parameters
    ----------
    bbox : A bounding box for the query
    timeout : HTTP request timeout in seconds
    force_no_data_value: If not None, use this value as no data value for the requested raster data. \
        Otherwise, use the Geo Engine will produce masked rasters.
    &#39;&#39;&#39;

    with self.__get_wcs_tiff_as_memory_file(
        bbox,
        timeout,
        force_no_data_value
    ) as memfile, memfile.open() as dataset:
        data_array = rioxarray.open_rasterio(dataset)

        # helping mypy with inference
        assert isinstance(data_array, xr.DataArray)

        rio: xr.DataArray = data_array.rio
        rio.update_attrs({
            &#39;crs&#39;: rio.crs,
            &#39;res&#39;: rio.resolution(),
            &#39;transform&#39;: rio.transform(),
        }, inplace=True)

        # TODO: add time information to dataset
        return data_array.load()</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.metadata_zip"><code class="name flex">
<span>def <span class="ident">metadata_zip</span></span>(<span>self, path: Union[PathLike, BytesIO], timeout: int = 60) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Query workflow metadata and citations and stores it as zip file to <code>path</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def metadata_zip(self, path: Union[PathLike, BytesIO], timeout: int = 60) -&gt; None:
    &#39;&#39;&#39;
    Query workflow metadata and citations and stores it as zip file to `path`
    &#39;&#39;&#39;

    session = get_session()

    provenance_url = f&#39;{session.server_url}/workflow/{self.__workflow_id}/allMetadata/zip&#39;

    response = req.get(provenance_url, headers=session.auth_header, timeout=timeout).content

    if isinstance(path, BytesIO):
        path.write(response)
    else:
        with open(path, &#39;wb&#39;) as file:
            file.write(response)</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.plot_chart"><code class="name flex">
<span>def <span class="ident">plot_chart</span></span>(<span>self, bbox: QueryRectangle, timeout: int = 3600) ‑> vega.vegalite.VegaLite</span>
</code></dt>
<dd>
<div class="desc"><p>Query a workflow and return the plot chart result as a vega plot</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_chart(self, bbox: QueryRectangle, timeout: int = 3600) -&gt; VegaLite:
    &#39;&#39;&#39;
    Query a workflow and return the plot chart result as a vega plot
    &#39;&#39;&#39;

    if not self.__result_descriptor.is_plot_result():
        raise MethodNotCalledOnPlotException()

    session = get_session()

    time = urllib.parse.quote(bbox.time_str)
    spatial_bounds = urllib.parse.quote(bbox.bbox_str)
    resolution = str(bbox.spatial_resolution)

    plot_url = f&#39;{session.server_url}/plot/{self}?bbox={spatial_bounds}&amp;crs={bbox.srs}&amp;time={time}&#39;\
        f&#39;&amp;spatialResolution={resolution}&#39;

    response = req.get(plot_url, headers=session.auth_header, timeout=timeout)

    check_response_for_error(response)

    response_json: JsonType = response.json()
    assert isinstance(response_json, Dict)

    vega_spec: VegaSpec = json.loads(response_json[&#39;data&#39;][&#39;vegaString&#39;])

    return VegaLite(vega_spec)</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.raster_stream"><code class="name flex">
<span>async def <span class="ident">raster_stream</span></span>(<span>self, query_rectangle: QueryRectangle, clip_to_query_rectangle: bool = False, open_timeout: int = 60) ‑> AsyncIterator[xarray.core.dataarray.DataArray]</span>
</code></dt>
<dd>
<div class="desc"><p>Stream the workflow result as series of xarrays</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def raster_stream(
        self,
        query_rectangle: QueryRectangle,
        clip_to_query_rectangle: bool = False,
        open_timeout: int = 60) -&gt; AsyncIterator[xr.DataArray]:
    &#39;&#39;&#39;Stream the workflow result as series of xarrays&#39;&#39;&#39;

    def read_arrow_ipc(arrow_ipc: bytes) -&gt; pa.RecordBatch:
        reader = pa.ipc.open_file(arrow_ipc)
        # We know from the backend that there is only one record batch
        record_batch = reader.get_record_batch(0)
        return record_batch

    def create_xarray(record_batch: pa.RecordBatch) -&gt; xr.DataArray:
        metadata = record_batch.schema.metadata
        geo_transform: GeoTransform = GeoTransform.from_response(json.loads(metadata[b&#39;geoTransform&#39;]))
        x_size = int(metadata[b&#39;xSize&#39;])
        y_size = int(metadata[b&#39;ySize&#39;])
        spatial_reference = metadata[b&#39;spatialReference&#39;].decode(&#39;utf-8&#39;)
        # We know from the backend that there is only one array a.k.a. one column
        arrow_array = record_batch.column(0)

        time = TimeInterval.from_response(json.loads(metadata[b&#39;time&#39;]))

        array = xr.DataArray(
            arrow_array.to_numpy(
                zero_copy_only=False,  # cannot zero-copy as soon as we have nodata values
            ).reshape(x_size, y_size),
            dims=[&#34;y&#34;, &#34;x&#34;],
            coords={
                &#39;x&#39;: np.arange(
                    start=geo_transform.x_min + geo_transform.x_half_pixel_size,
                    step=geo_transform.x_pixel_size,
                    stop=geo_transform.x_max(x_size),
                ),
                &#39;y&#39;: np.arange(
                    start=geo_transform.y_max + geo_transform.y_half_pixel_size,
                    step=geo_transform.y_pixel_size,
                    stop=geo_transform.y_min(y_size),
                ),
                &#39;time&#39;: time.start,  # TODO: incorporate time end?
            },
        )

        array.rio.write_crs(spatial_reference, inplace=True)

        return array

    def process_bytes(tile_bytes: Optional[bytes]) -&gt; Optional[xr.DataArray]:
        if tile_bytes is None:
            return None

        # process the received data
        record_batch = read_arrow_ipc(tile_bytes)
        tile = create_xarray(record_batch)

        return tile

    # Currently, it only works for raster results
    if not self.__result_descriptor.is_raster_result():
        raise MethodNotCalledOnRasterException()

    session = get_session()

    url = req.Request(
        &#39;GET&#39;,
        url=f&#39;{session.server_url}/workflow/{self.__workflow_id}/rasterStream&#39;,
        params={
            &#39;resultType&#39;: &#39;arrow&#39;,
            &#39;spatialBounds&#39;: query_rectangle.bbox_str,
            &#39;timeInterval&#39;: query_rectangle.time_str,
            &#39;spatialResolution&#39;: str(query_rectangle.spatial_resolution),
        },
    ).prepare().url

    if url is None:
        raise InputException(&#39;Invalid websocket url&#39;)

    # for the websockets library, it is necessary that the url starts with `ws://``
    [_, url_part] = url.split(&#39;://&#39;, maxsplit=1)

    async with websockets.client.connect(
        uri=f&#39;ws://{url_part}&#39;,
        extra_headers=session.auth_header,
        open_timeout=open_timeout,
    ) as websocket:

        tile_bytes: Optional[bytes] = None

        while websocket.open:
            async def read_new_bytes() -&gt; Optional[bytes]:
                # already send the next request to speed up the process
                try:
                    await websocket.send(&#34;NEXT&#34;)
                except websockets.exceptions.ConnectionClosed:
                    # the websocket connection is already closed, we cannot read anymore
                    return None

                try:
                    data: Union[str, bytes] = await websocket.recv()

                    if isinstance(data, str):
                        # the server sent an error message
                        raise GeoEngineException({&#39;error&#39;: data})

                    return data
                except websockets.exceptions.ConnectionClosedOK:
                    # the websocket connection closed gracefully, so we stop reading
                    return None

            (tile_bytes, tile) = await asyncio.gather(
                read_new_bytes(),
                # asyncio.to_thread(process_bytes, tile_bytes), # TODO: use this when min Python version is 3.9
                backports.to_thread(process_bytes, tile_bytes),
            )

            if tile is not None:
                if clip_to_query_rectangle:
                    tile = tile.rio.clip_box(*query_rectangle.spatial_bounds.as_bbox_tuple())
                    tile = cast(xr.DataArray, tile)

                yield tile

        # process the last tile
        tile = process_bytes(tile_bytes)

        if tile is not None:
            if clip_to_query_rectangle:
                tile = tile.rio.clip_box(*query_rectangle.spatial_bounds.as_bbox_tuple())
                tile = cast(xr.DataArray, tile)

            yield tile</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.raster_stream_into_xarray"><code class="name flex">
<span>async def <span class="ident">raster_stream_into_xarray</span></span>(<span>self, query_rectangle: QueryRectangle, clip_to_query_rectangle: bool = False, open_timeout: int = 60) ‑> xarray.core.dataarray.DataArray</span>
</code></dt>
<dd>
<div class="desc"><p>Stream the workflow result into memory and output a single xarray.</p>
<p>NOTE: You can run out of memory if the query rectangle is too large.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def raster_stream_into_xarray(
        self,
        query_rectangle: QueryRectangle,
        clip_to_query_rectangle: bool = False,
        open_timeout: int = 60) -&gt; xr.DataArray:
    &#39;&#39;&#39;
    Stream the workflow result into memory and output a single xarray.

    NOTE: You can run out of memory if the query rectangle is too large.
    &#39;&#39;&#39;

    tile_stream = self.raster_stream(
        query_rectangle,
        clip_to_query_rectangle=clip_to_query_rectangle,
        open_timeout=open_timeout
    )

    timesteps: List[xr.DataArray] = []

    async def read_tiles(
        remainder_tile: Optional[xr.DataArray]
    ) -&gt; tuple[List[xr.DataArray], Optional[xr.DataArray]]:
        last_timestep: Optional[np.datetime64] = None
        tiles = []

        if remainder_tile is not None:
            last_timestep = remainder_tile.time.values
            tiles.append(remainder_tile)

        async for tile in tile_stream:
            timestep: np.datetime64 = tile.time.values
            if last_timestep is None:
                last_timestep = timestep
            elif last_timestep != timestep:
                return tiles, tile

            tiles.append(tile)

        # this seems to be the last time step, so just return tiles
        return tiles, None

    def merge_tiles(tiles: List[xr.DataArray]) -&gt; Optional[xr.DataArray]:
        if len(tiles) == 0:
            return None

        combined_tiles = xr.combine_by_coords(tiles)

        if isinstance(combined_tiles, xr.Dataset):
            raise TypeException(&#39;Internal error: Merging data arrays should result in a data array.&#39;)

        return combined_tiles

    (tiles, remainder_tile) = await read_tiles(None)

    while len(tiles):
        ((new_tiles, new_remainder_tile), new_timestep) = await asyncio.gather(
            read_tiles(remainder_tile),
            backports.to_thread(merge_tiles, tiles)
            # asyncio.to_thread(merge_tiles, tiles), # TODO: use this when min Python version is 3.9
        )

        tiles = new_tiles
        remainder_tile = new_remainder_tile

        if new_timestep is not None:
            timesteps.append(new_timestep)

    output: xr.DataArray = cast(
        xr.DataArray,
        # await asyncio.to_thread( # TODO: use this when min Python version is 3.9
        await backports.to_thread(
            xr.concat,
            # TODO: This is a typings error, since the method accepts also a `xr.DataArray` and returns one
            cast(List[xr.Dataset], timesteps),
            dim=&#39;time&#39;
        )
    )

    return output</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.save_as_dataset"><code class="name flex">
<span>def <span class="ident">save_as_dataset</span></span>(<span>self, query_rectangle: api.RasterQueryRectangle, name: str, description: str = '', timeout: int = 3600) ‑> <a title="geoengine.tasks.Task" href="tasks.html#geoengine.tasks.Task">Task</a></span>
</code></dt>
<dd>
<div class="desc"><p>Init task to store the workflow result as a layer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_as_dataset(
        self,
        query_rectangle: api.RasterQueryRectangle,
        name: str,
        description: str = &#39;&#39;,
        timeout: int = 3600) -&gt; Task:
    &#39;&#39;&#39;Init task to store the workflow result as a layer&#39;&#39;&#39;

    # Currently, it only works for raster results
    if not self.__result_descriptor.is_raster_result():
        raise MethodNotCalledOnRasterException()

    session = get_session()

    request_body = {
        &#39;name&#39;: name,
        &#39;description&#39;: description,
        &#39;query&#39;: query_rectangle,
    }

    response = req.post(
        url=f&#39;{session.server_url}/datasetFromWorkflow/{self.__workflow_id}&#39;,
        json=request_body,
        headers=session.auth_header,
        timeout=timeout
    )

    check_response_for_error(response)

    return Task(TaskId.from_response(response.json()))</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.vector_stream"><code class="name flex">
<span>async def <span class="ident">vector_stream</span></span>(<span>self, query_rectangle: QueryRectangle, time_start_column: str = 'time_start', time_end_column: str = 'time_end', open_timeout: int = 60) ‑> AsyncIterator[geopandas.geodataframe.GeoDataFrame]</span>
</code></dt>
<dd>
<div class="desc"><p>Stream the workflow result as series of <code>GeoDataFrame</code>s</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def vector_stream(
        self,
        query_rectangle: QueryRectangle,
        time_start_column: str = &#39;time_start&#39;,
        time_end_column: str = &#39;time_end&#39;,
        open_timeout: int = 60) -&gt; AsyncIterator[gpd.GeoDataFrame]:
    &#39;&#39;&#39;Stream the workflow result as series of `GeoDataFrame`s&#39;&#39;&#39;

    def read_arrow_ipc(arrow_ipc: bytes) -&gt; pa.RecordBatch:
        reader = pa.ipc.open_file(arrow_ipc)
        # We know from the backend that there is only one record batch
        record_batch = reader.get_record_batch(0)
        return record_batch

    def create_geo_data_frame(record_batch: pa.RecordBatch,
                              time_start_column: str,
                              time_end_column: str) -&gt; gpd.GeoDataFrame:
        metadata = record_batch.schema.metadata
        spatial_reference = metadata[b&#39;spatialReference&#39;].decode(&#39;utf-8&#39;)

        data_frame = record_batch.to_pandas()

        geometry = gpd.GeoSeries.from_wkt(data_frame[api.GEOMETRY_COLUMN_NAME])
        del data_frame[api.GEOMETRY_COLUMN_NAME]  # delete the duplicated column

        geo_data_frame = gpd.GeoDataFrame(
            data_frame,
            geometry=geometry,
            crs=spatial_reference,
        )

        # split time column
        geo_data_frame[[time_start_column, time_end_column]] = geo_data_frame[api.TIME_COLUMN_NAME].tolist()
        del geo_data_frame[api.TIME_COLUMN_NAME]  # delete the duplicated column

        # parse time columns
        for time_column in [time_start_column, time_end_column]:
            geo_data_frame[time_column] = pd.to_datetime(
                geo_data_frame[time_column],
                utc=True,
                unit=&#39;ms&#39;,
                # TODO: solve time conversion problem from Geo Engine to Python for large (+/-) time instances
                errors=&#39;coerce&#39;,
            )

        return geo_data_frame

    def process_bytes(batch_bytes: Optional[bytes]) -&gt; Optional[gpd.GeoDataFrame]:
        if batch_bytes is None:
            return None

        # process the received data
        record_batch = read_arrow_ipc(batch_bytes)
        tile = create_geo_data_frame(
            record_batch,
            time_start_column=time_start_column,
            time_end_column=time_end_column,
        )

        return tile

    # Currently, it only works for raster results
    if not self.__result_descriptor.is_vector_result():
        raise MethodNotCalledOnVectorException()

    session = get_session()

    url = req.Request(
        &#39;GET&#39;,
        url=f&#39;{session.server_url}/workflow/{self.__workflow_id}/vectorStream&#39;,
        params={
            &#39;resultType&#39;: &#39;arrow&#39;,
            &#39;spatialBounds&#39;: query_rectangle.bbox_str,
            &#39;timeInterval&#39;: query_rectangle.time_str,
            &#39;spatialResolution&#39;: str(query_rectangle.spatial_resolution),
        },
    ).prepare().url

    if url is None:
        raise InputException(&#39;Invalid websocket url&#39;)

    # for the websockets library, it is necessary that the url starts with `ws://``
    [_, url_part] = url.split(&#39;://&#39;, maxsplit=1)

    async with websockets.client.connect(
        uri=f&#39;ws://{url_part}&#39;,
        extra_headers=session.auth_header,
        open_timeout=open_timeout,
        max_size=None,  # allow arbitrary large messages, since it is capped by the server&#39;s chunk size
    ) as websocket:

        batch_bytes: Optional[bytes] = None

        while websocket.open:
            async def read_new_bytes() -&gt; Optional[bytes]:
                # already send the next request to speed up the process
                try:
                    await websocket.send(&#34;NEXT&#34;)
                except websockets.exceptions.ConnectionClosed:
                    # the websocket connection is already closed, we cannot read anymore
                    return None

                try:
                    data: Union[str, bytes] = await websocket.recv()

                    if isinstance(data, str):
                        # the server sent an error message
                        raise GeoEngineException({&#39;error&#39;: data})

                    return data
                except websockets.exceptions.ConnectionClosedOK:
                    # the websocket connection closed gracefully, so we stop reading
                    return None

            (batch_bytes, batch) = await asyncio.gather(
                read_new_bytes(),
                # asyncio.to_thread(process_bytes, batch_bytes), # TODO: use this when min Python version is 3.9
                backports.to_thread(process_bytes, batch_bytes),
            )

            if batch is not None:
                yield batch

        # process the last tile
        batch = process_bytes(batch_bytes)

        if batch is not None:
            yield batch</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.vector_stream_into_geopandas"><code class="name flex">
<span>async def <span class="ident">vector_stream_into_geopandas</span></span>(<span>self, query_rectangle: QueryRectangle, time_start_column: str = 'time_start', time_end_column: str = 'time_end', open_timeout: int = 60) ‑> geopandas.geodataframe.GeoDataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Stream the workflow result into memory and output a single geo data frame.</p>
<p>NOTE: You can run out of memory if the query rectangle is too large.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def vector_stream_into_geopandas(
        self,
        query_rectangle: QueryRectangle,
        time_start_column: str = &#39;time_start&#39;,
        time_end_column: str = &#39;time_end&#39;,
        open_timeout: int = 60) -&gt; gpd.GeoDataFrame:
    &#39;&#39;&#39;
    Stream the workflow result into memory and output a single geo data frame.

    NOTE: You can run out of memory if the query rectangle is too large.
    &#39;&#39;&#39;

    chunk_stream = self.vector_stream(
        query_rectangle,
        time_start_column=time_start_column,
        time_end_column=time_end_column,
        open_timeout=open_timeout,
    )

    data_frame: Optional[gpd.GeoDataFrame] = None
    chunk: Optional[gpd.GeoDataFrame] = None

    async def read_dataframe() -&gt; Optional[gpd.GeoDataFrame]:
        try:
            return await chunk_stream.__anext__()
        except StopAsyncIteration:
            return None

    def merge_dataframes(
        df_a: Optional[gpd.GeoDataFrame],
        df_b: Optional[gpd.GeoDataFrame]
    ) -&gt; Optional[gpd.GeoDataFrame]:
        if df_a is None:
            return df_b

        if df_b is None:
            return df_a

        return pd.concat([df_a, df_b], ignore_index=True)

    while True:
        (chunk, data_frame) = await asyncio.gather(
            read_dataframe(),
            backports.to_thread(merge_dataframes, data_frame, chunk),
            # TODO: use this when min Python version is 3.9
            # asyncio.to_thread(merge_dataframes, data_frame, chunk),
        )

        # we can stop when the chunk stream is exhausted
        if chunk is None:
            break

    return data_frame</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.wms_get_map_as_image"><code class="name flex">
<span>def <span class="ident">wms_get_map_as_image</span></span>(<span>self, bbox: QueryRectangle, colorizer: Colorizer) ‑> Image</span>
</code></dt>
<dd>
<div class="desc"><p>Return the result of a WMS request as a PIL Image</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wms_get_map_as_image(self, bbox: QueryRectangle, colorizer: Colorizer) -&gt; Image:
    &#39;&#39;&#39;Return the result of a WMS request as a PIL Image&#39;&#39;&#39;

    wms_request = self.__wms_get_map_request(bbox, colorizer)
    response = req.Session().send(wms_request)

    check_response_for_error(response)

    return Image.open(BytesIO(response.content))</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.wms_get_map_curl"><code class="name flex">
<span>def <span class="ident">wms_get_map_curl</span></span>(<span>self, bbox: QueryRectangle, colorizer: Colorizer) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Return the WMS curl command for a workflow and a given <code>QueryRectangle</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wms_get_map_curl(self, bbox: QueryRectangle, colorizer: Colorizer) -&gt; str:
    &#39;&#39;&#39;Return the WMS curl command for a workflow and a given `QueryRectangle`&#39;&#39;&#39;

    wms_request = self.__wms_get_map_request(bbox, colorizer)

    command = &#34;curl -X {method} -H {headers} &#39;{uri}&#39;&#34;
    headers_list = [f&#39;&#34;{k}: {v}&#34;&#39; for k, v in wms_request.headers.items()]
    headers = &#34; -H &#34;.join(headers_list)
    return command.format(method=wms_request.method, headers=headers, uri=wms_request.url)</code></pre>
</details>
</dd>
<dt id="geoengine.workflow.Workflow.workflow_definition"><code class="name flex">
<span>def <span class="ident">workflow_definition</span></span>(<span>self, timeout: int = 60) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Return the workflow definition for this workflow</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def workflow_definition(self, timeout: int = 60) -&gt; Dict[str, Any]:
    &#39;&#39;&#39;Return the workflow definition for this workflow&#39;&#39;&#39;

    session = get_session()

    response = req.get(
        f&#39;{session.server_url}/workflow/{self.__workflow_id}&#39;,
        headers=session.auth_header,
        timeout=timeout
    ).json()

    return response</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="geoengine.workflow.WorkflowId"><code class="flex name class">
<span>class <span class="ident">WorkflowId</span></span>
<span>(</span><span>workflow_id: UUID)</span>
</code></dt>
<dd>
<div class="desc"><p>A wrapper around a workflow UUID</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WorkflowId:
    &#39;&#39;&#39;
    A wrapper around a workflow UUID
    &#39;&#39;&#39;

    __workflow_id: UUID

    def __init__(self, workflow_id: UUID) -&gt; None:
        self.__workflow_id = workflow_id

    @classmethod
    def from_response(cls, response: api.WorkflowId) -&gt; WorkflowId:
        &#39;&#39;&#39;
        Create a `WorkflowId` from an http response
        &#39;&#39;&#39;
        if &#39;id&#39; not in response:
            raise TypeError(&#39;Response does not contain a workflow id.&#39;)
        return WorkflowId(UUID(response[&#39;id&#39;]))

    def __str__(self) -&gt; str:
        return str(self.__workflow_id)

    def __repr__(self) -&gt; str:
        return str(self)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="geoengine.workflow.WorkflowId.from_response"><code class="name flex">
<span>def <span class="ident">from_response</span></span>(<span>response: api.WorkflowId) ‑> <a title="geoengine.workflow.WorkflowId" href="#geoengine.workflow.WorkflowId">WorkflowId</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create a <code><a title="geoengine.workflow.WorkflowId" href="#geoengine.workflow.WorkflowId">WorkflowId</a></code> from an http response</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_response(cls, response: api.WorkflowId) -&gt; WorkflowId:
    &#39;&#39;&#39;
    Create a `WorkflowId` from an http response
    &#39;&#39;&#39;
    if &#39;id&#39; not in response:
        raise TypeError(&#39;Response does not contain a workflow id.&#39;)
    return WorkflowId(UUID(response[&#39;id&#39;]))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="geoengine.workflow.X"><code class="flex name class">
<span>class <span class="ident">X</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="geoengine.workflow.X.axis"><code class="name">var <span class="ident">axis</span> : <a title="geoengine.workflow.Axis" href="#geoengine.workflow.Axis">Axis</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="geoengine.workflow.X.bin"><code class="name">var <span class="ident">bin</span> : <a title="geoengine.workflow.Bin" href="#geoengine.workflow.Bin">Bin</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="geoengine.workflow.X.field"><code class="name">var <span class="ident">field</span> : <a title="geoengine.workflow.Field" href="#geoengine.workflow.Field">Field</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="geoengine.workflow.X2"><code class="flex name class">
<span>class <span class="ident">X2</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="geoengine.workflow.X2.field"><code class="name">var <span class="ident">field</span> : <a title="geoengine.workflow.Field" href="#geoengine.workflow.Field">Field</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="geoengine.workflow.Y"><code class="flex name class">
<span>class <span class="ident">Y</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="geoengine.workflow.Y.field"><code class="name">var <span class="ident">field</span> : <a title="geoengine.workflow.Field" href="#geoengine.workflow.Field">Field</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="geoengine.workflow.Y.type"><code class="name">var <span class="ident">type</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="geoengine" href="index.html">geoengine</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="geoengine.workflow.get_quota" href="#geoengine.workflow.get_quota">get_quota</a></code></li>
<li><code><a title="geoengine.workflow.register_workflow" href="#geoengine.workflow.register_workflow">register_workflow</a></code></li>
<li><code><a title="geoengine.workflow.update_quota" href="#geoengine.workflow.update_quota">update_quota</a></code></li>
<li><code><a title="geoengine.workflow.workflow_by_id" href="#geoengine.workflow.workflow_by_id">workflow_by_id</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="geoengine.workflow.Axis" href="#geoengine.workflow.Axis">Axis</a></code></h4>
<ul class="">
<li><code><a title="geoengine.workflow.Axis.title" href="#geoengine.workflow.Axis.title">title</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="geoengine.workflow.Bin" href="#geoengine.workflow.Bin">Bin</a></code></h4>
<ul class="">
<li><code><a title="geoengine.workflow.Bin.binned" href="#geoengine.workflow.Bin.binned">binned</a></code></li>
<li><code><a title="geoengine.workflow.Bin.step" href="#geoengine.workflow.Bin.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="geoengine.workflow.DatasetIds" href="#geoengine.workflow.DatasetIds">DatasetIds</a></code></h4>
<ul class="">
<li><code><a title="geoengine.workflow.DatasetIds.dataset" href="#geoengine.workflow.DatasetIds.dataset">dataset</a></code></li>
<li><code><a title="geoengine.workflow.DatasetIds.upload" href="#geoengine.workflow.DatasetIds.upload">upload</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="geoengine.workflow.Encoding" href="#geoengine.workflow.Encoding">Encoding</a></code></h4>
<ul class="">
<li><code><a title="geoengine.workflow.Encoding.x" href="#geoengine.workflow.Encoding.x">x</a></code></li>
<li><code><a title="geoengine.workflow.Encoding.x2" href="#geoengine.workflow.Encoding.x2">x2</a></code></li>
<li><code><a title="geoengine.workflow.Encoding.y" href="#geoengine.workflow.Encoding.y">y</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="geoengine.workflow.Field" href="#geoengine.workflow.Field">Field</a></code></h4>
<ul class="">
<li><code><a title="geoengine.workflow.Field.field" href="#geoengine.workflow.Field.field">field</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="geoengine.workflow.Values" href="#geoengine.workflow.Values">Values</a></code></h4>
<ul class="">
<li><code><a title="geoengine.workflow.Values.Frequency" href="#geoengine.workflow.Values.Frequency">Frequency</a></code></li>
<li><code><a title="geoengine.workflow.Values.binEnd" href="#geoengine.workflow.Values.binEnd">binEnd</a></code></li>
<li><code><a title="geoengine.workflow.Values.binStart" href="#geoengine.workflow.Values.binStart">binStart</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="geoengine.workflow.VegaSpec" href="#geoengine.workflow.VegaSpec">VegaSpec</a></code></h4>
<ul class="">
<li><code><a title="geoengine.workflow.VegaSpec.$schema" href="#geoengine.workflow.VegaSpec.$schema">$schema</a></code></li>
<li><code><a title="geoengine.workflow.VegaSpec.data" href="#geoengine.workflow.VegaSpec.data">data</a></code></li>
<li><code><a title="geoengine.workflow.VegaSpec.encoding" href="#geoengine.workflow.VegaSpec.encoding">encoding</a></code></li>
<li><code><a title="geoengine.workflow.VegaSpec.mark" href="#geoengine.workflow.VegaSpec.mark">mark</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="geoengine.workflow.Workflow" href="#geoengine.workflow.Workflow">Workflow</a></code></h4>
<ul class="">
<li><code><a title="geoengine.workflow.Workflow.download_raster" href="#geoengine.workflow.Workflow.download_raster">download_raster</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.get_array" href="#geoengine.workflow.Workflow.get_array">get_array</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.get_dataframe" href="#geoengine.workflow.Workflow.get_dataframe">get_dataframe</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.get_provenance" href="#geoengine.workflow.Workflow.get_provenance">get_provenance</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.get_result_descriptor" href="#geoengine.workflow.Workflow.get_result_descriptor">get_result_descriptor</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.get_wfs_get_feature_curl" href="#geoengine.workflow.Workflow.get_wfs_get_feature_curl">get_wfs_get_feature_curl</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.get_xarray" href="#geoengine.workflow.Workflow.get_xarray">get_xarray</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.metadata_zip" href="#geoengine.workflow.Workflow.metadata_zip">metadata_zip</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.plot_chart" href="#geoengine.workflow.Workflow.plot_chart">plot_chart</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.raster_stream" href="#geoengine.workflow.Workflow.raster_stream">raster_stream</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.raster_stream_into_xarray" href="#geoengine.workflow.Workflow.raster_stream_into_xarray">raster_stream_into_xarray</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.save_as_dataset" href="#geoengine.workflow.Workflow.save_as_dataset">save_as_dataset</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.vector_stream" href="#geoengine.workflow.Workflow.vector_stream">vector_stream</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.vector_stream_into_geopandas" href="#geoengine.workflow.Workflow.vector_stream_into_geopandas">vector_stream_into_geopandas</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.wms_get_map_as_image" href="#geoengine.workflow.Workflow.wms_get_map_as_image">wms_get_map_as_image</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.wms_get_map_curl" href="#geoengine.workflow.Workflow.wms_get_map_curl">wms_get_map_curl</a></code></li>
<li><code><a title="geoengine.workflow.Workflow.workflow_definition" href="#geoengine.workflow.Workflow.workflow_definition">workflow_definition</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="geoengine.workflow.WorkflowId" href="#geoengine.workflow.WorkflowId">WorkflowId</a></code></h4>
<ul class="">
<li><code><a title="geoengine.workflow.WorkflowId.from_response" href="#geoengine.workflow.WorkflowId.from_response">from_response</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="geoengine.workflow.X" href="#geoengine.workflow.X">X</a></code></h4>
<ul class="">
<li><code><a title="geoengine.workflow.X.axis" href="#geoengine.workflow.X.axis">axis</a></code></li>
<li><code><a title="geoengine.workflow.X.bin" href="#geoengine.workflow.X.bin">bin</a></code></li>
<li><code><a title="geoengine.workflow.X.field" href="#geoengine.workflow.X.field">field</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="geoengine.workflow.X2" href="#geoengine.workflow.X2">X2</a></code></h4>
<ul class="">
<li><code><a title="geoengine.workflow.X2.field" href="#geoengine.workflow.X2.field">field</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="geoengine.workflow.Y" href="#geoengine.workflow.Y">Y</a></code></h4>
<ul class="">
<li><code><a title="geoengine.workflow.Y.field" href="#geoengine.workflow.Y.field">field</a></code></li>
<li><code><a title="geoengine.workflow.Y.type" href="#geoengine.workflow.Y.type">type</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>