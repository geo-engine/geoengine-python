{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-step joining of monthly Sentinel-2 data with points of interest for training and application of a Random-Forest model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Introduction\n",
        "\n",
        "This notebook demonstrates how to use the Geo Engine to process raster and vector data to to train an external Random-Forest model for field-use classification.\n",
        "The generation of the training data is dnoe in the Geo Engine.\n",
        "The model is trained using sklearn in this notebook.\n",
        "Then, the model is applied to Sentinel-2 data queried from the Geo Engine.\n",
        "\n",
        "### Use-Case\n",
        "\n",
        "Spatial information of field-use is very important for various applications like yield estimation.\n",
        "Using Earth Observation (EO) data we can generate large raster maps of crop type or field-use.\n",
        "In this notebook, we want to generate such a map for an area in the state of North Rhine-Westphalia (NRW), Germany.\n",
        "To generate such a map, we need ground truth data, e.g. sampling points where the real field-use is known.\n",
        "Then, we can train a Maschine Learning (ML) modell on the sample points and the corresponding values of the EO data.\n",
        "The trained model can then be applied to the whole area to generate a map of the field-use.\n",
        "\n",
        "### Data\n",
        "\n",
        "In this notebook, we use the following data:\n",
        "\n",
        "#### EuroCrops (NRW, Germany)\n",
        "\n",
        "As label data (ground truth) we use the EuroCrops data for NRW, Germany.\n",
        "It is available under the [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).\n",
        "The data can be downloaded from [here](https://github.com/maja601/EuroCrops#vectordata_zenodo).\n",
        "This dataset contains field polygons with different classes e.g. the crop type or the usage of the field.\n",
        "For the demo presented in this notebook, we convert the field polygons into points and use them as sample-points.\n",
        "\n",
        "#### Sentinel-2\n",
        "\n",
        "The Sentinel-2 data we use is available from the Element 84 Sentinel-2 L2A Data Hub.\n",
        "Using the STAC API, it can be queried for areas and times of interest.\n",
        "The data is available under the [ESA Data License](https://sentinel.esa.int/documents/247904/690755/Sentinel_Data_Legal_Notice).\n",
        "The Geo Engine provides an [ExternalDataProvider] that acts as a STAC API client that can be used to query the data.\n",
        "\n",
        "### Workflow of this notebook\n",
        "\n",
        "This notebook focuses on the extraction of the Sentinel-2 data for the points of interest and the model training :\n",
        "1. Setup of packages, the Geo Engnie session and the area of interest.\n",
        "2. Download the Sentinel-2 data for the area of interest and store it as a new (local) dataset.\n",
        "3. Build the workflow for cloud-free monthly means of Sentinel-2 data and derive the NDVI.\n",
        "4. Attach the aggregated Sentinel-2 to the points of interest.\n",
        "5. Train a Random-Forest model to classify the field usage based on the Sentinel-2 data attached to the points.\n",
        "6. Apply the model to Sentinel-2 data queried from the Geo Engine.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook requires the following packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import asyncio\n",
        "import geopandas as gpd\n",
        "import geoengine as ge\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import rioxarray\n",
        "from asyncstdlib.itertools import zip_longest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Connect to a Geo Engine instance. You need to provide user credentials via parameters or environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ge.initialize(\"http://localhost:3030/api\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "session = ge.get_session()\n",
        "user_id = session.user_id\n",
        "session"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the area of interest. It is defined as a bounding box in UTM 32 N (EPSG:32632).\n",
        "It is locted in NRW, Germany and covers the area between Willingen, Lippstadt and Werl."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[xmin, ymin, xmax, ymax] = [421395,  5681078, 476201, 5727833]\n",
        "size_x = xmax - xmin\n",
        "size_y = ymax - ymin\n",
        "print(size_x, size_y)\n",
        "(xmin, ymin, xmax, ymax)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the bounding box, a time interval and a resolution, we define the area of interest as a temporal raster space-time cube.\n",
        "The time interval is defined as a start and end date.\n",
        "Since the field data is available for the year 2018, we use this year as the time interval.\n",
        "The Sentinel-2 data has a resolution of 10 m, so we use this resolution for the area of interest as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_start = datetime(2021, 1, 1)\n",
        "time_end = datetime(2022, 1, 1)\n",
        "\n",
        "study_area = ge.api.RasterQueryRectangle(\n",
        "    spatialBounds=ge.SpatialPartition2D(xmin, ymin, xmax, ymax).to_api_dict(),\n",
        "    timeInterval=ge.TimeInterval(time_start, time_end).to_api_dict(),\n",
        "    spatialResolution=ge.SpatialResolution(100.0, 100.0).to_api_dict(),\n",
        ")\n",
        "study_area"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download Sentinel-2 data and store it in the Geo Engine\n",
        "\n",
        "The Sentinel-2 data is stored in the cloud (AWS S3).\n",
        "There is a STAC API that provides access to the data. \n",
        "To use the data in the Geo Engine, we create workflows that accesses the different Sentinel-2 bands and make them available.\n",
        "This first kind of workflow only queries the data in the area of interest and stores it as local datasets.\n",
        "This way, we don't need to download the data every time we use it."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each band (B02, B03, B04, B08) as well as the scene mask (SCL), we create a workflow using the `sentinel2_band(band_name)` blueprint.\n",
        "This convenience method creates a workflow that uses the [`GdalSource`](https://docs.geoengine.io/operators/gdalsource.html) to load the Sentinel-2 data.\n",
        "A source operator like the `GdalSource` takes a `DatasetId` that identifies the data to load.\n",
        "In this case, the `DatasetId` is provided by an external dataset provider, that resolves band \"names\" to the information how to load the data from STAC / S3.\n",
        "Using the `save_as_dataset(study_area)` method, we tell the Geo Engine to store the data as a new dataset.\n",
        "The new dataset gets a unique `DatasetId` that we can use to load the data in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "download_tasks = {}\n",
        "s2_data_prefix = user_id + \":y_sentinel2_nrw_crop_10m_\"\n",
        "\n",
        "for b in [\"B02\", \"B03\", \"B04\", \"B08\", \"SCL\"]:\n",
        "    sentinel2_band_workflow = ge.workflow_builder.blueprints.sentinel2_band(b)\n",
        "    sentinel2_band_workflow_id = ge.register_workflow(sentinel2_band_workflow)\n",
        "    sentinel2_band_workflow_dataset_task = sentinel2_band_workflow_id.save_as_dataset(study_area, f\"{s2_data_prefix}{b}\", f\"Sentinel-2 NRW area 10m {b}\")\n",
        "    # We start the download task and turn it into a future. This way we can await all tasks at once.\n",
        "    download_tasks[b] = sentinel2_band_workflow_dataset_task.as_future(print_status=False, request_interval=60)\n",
        "\n",
        "# the asyncio.gather function awaits all tasks at once and returns the final status as a list\n",
        "download_task_results = await asyncio.gather(*download_tasks.values())\n",
        "\n",
        "download_task_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## The datasets have been downloaded and have been registered with Geo Engine. We can now use them in a workflow.\n",
        "\n",
        "## Either convert the list of download results into a dictionary with the band name as key:\n",
        "\n",
        "# band_dataset_names = {band: task_result.info['dataset'] for band, task_result in zip(download_tasks.keys(), download_task_results)}\n",
        "# band_dataset_names\n",
        "\n",
        "## Or just use the dataset names as defined in the download step:\n",
        "\n",
        "band_dataset_names = {\n",
        " 'B02': user_id + ':y_sentinel2_nrw_crop_10m_B02',\n",
        " 'B03': user_id + ':y_sentinel2_nrw_crop_10m_B03',\n",
        " 'B04': user_id + ':y_sentinel2_nrw_crop_10m_B04',\n",
        " 'B08': user_id + ':y_sentinel2_nrw_crop_10m_B08',\n",
        " 'SCL': user_id + ':y_sentinel2_nrw_crop_10m_SCL'\n",
        "}\n",
        "\n",
        "band_dataset_names"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Monthly, cloud-free aggregations of the Sentinel-2 bands & NDVI\n",
        "\n",
        "For the training, we use the Sentinel-2 data of the bands 02, 03, 04, and 08. The scene classification layer (SCL) is used to filter out cloudy pixels for each band using an expression. The NDVI is calculated using an expression on band 4 and 8. \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now for each variable we want to use in the training, we create a workflow that aggregates the data to monthly, cloud-free means.\n",
        "The workflow is created using the `s2_cloud_free_aggregated_band_custom_input` method that can create the workflow for all the steps: loading data, removing clouded pixels, and aggregating them over the temporal domain.\n",
        "This method takes the `DatasetName` of the band of interest and the name of the scene classification layer (SCL) as input.\n",
        "\n",
        "It creates a workflow uses the [`GdalSource`](https://docs.geoengine.io/operators/gdalsource.html) to load both rasters and an [`Expression`](https://docs.geoengine.io/operators/expression.html) to filter out pixels that are marked \"cloudy\" in the SCL.\n",
        "The, now cloud-free, data is aggregated to monthly means using the [`TemporalRasterAggregation`](https://docs.geoengine.io/operators/temporalrasteraggregation.html) operator. You can also build that workflow using the operators directly.\n",
        "\n",
        "The NDVI is calculated using an expression on band 4 and 8.\n",
        "For convenience, there is a `s2_cloud_free_aggregated_ndvi_custom_input` method that takes the `DatasetName` of the bands 4, 8, and the SCL as input.\n",
        "It creates a workflow that loads the data, removes cloudy pixels and calculates the NDVI using an expression and then aggregates the data to monthly means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "monthly_cloud_free_workflows = {}\n",
        "\n",
        "for b in [\"B02\", \"B03\", \"B04\", \"B08\"]:\n",
        "    band_dataset_name = band_dataset_names[b]\n",
        "    scl_dataset_name = band_dataset_names[\"SCL\"]\n",
        "    sentinel2_band_workflow = ge.workflow_builder.blueprints.s2_cloud_free_aggregated_band_custom_input(band_id=band_dataset_name, scl_id=scl_dataset_name, granularity=\"months\", window_size=1, aggregation_type=\"mean\")\n",
        "    monthly_cloud_free_workflows[b] = sentinel2_band_workflow\n",
        "\n",
        "ndvi_workflow = ge.workflow_builder.blueprints.s2_cloud_free_aggregated_ndvi_custom_input(nir_dataset=band_dataset_names[\"B08\"], red_dataset=band_dataset_names[\"B04\"], scl_dataset=band_dataset_names[\"SCL\"], granularity=\"months\", window_size=1, aggregation_type=\"mean\")\n",
        "monthly_cloud_free_workflows[\"NDVI\"] = ndvi_workflow\n",
        "\n",
        "monthly_cloud_free_workflows"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the analysis of crops on fields, the temporal information is very important.\n",
        "Lots of information can be gained by looking at the development of the NDVI over time.\n",
        "Therefore, we want to generate for each point of interest the NDVI and other band information of the previous 8 months.\n",
        "To do this, we wrap the workflow in a [`TimeShift](https://docs.geoengine.io/operators/timeshift.html) operator.\n",
        "This way, we can use the existing workflows and just shift the temporal domain for all the months we are interested in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "monthly_cloud_free_workflows_shifted = {}\n",
        "\n",
        "for month_shift in range(0,-9, -1):\n",
        "    monthly_cloud_free_workflows_shifted[month_shift] = {b: ge.workflow_builder.operators.TimeShift(granularity=\"months\", value=month_shift, shift_type=\"relative\", source=x) for b, x in monthly_cloud_free_workflows.items()}\n",
        "    \n",
        "monthly_cloud_free_workflows_shifted\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Join the monthly Sentinel-2 data to the points of interest\n",
        "\n",
        "This step combines the monthly aggregated Sentinel-2 data with the points of interest. The resulting dataset is then queried from directly from python and stored as a pandas dataframe."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Upload the points of interest to the Geo Engine. First we use GeoPandas to load the points of interest into a DataFrame. Then we use the `upload_dataframe` method to upload the points to the Geo Engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points_df = gpd.read_file(\"group_sample_frac1_inspireId_use_utm32n.gpkg\")\n",
        "points_dataset_name = ge.upload_dataframe(points_df, \"group_sample_frac1_inspireId\")\n",
        "points_dataset_name"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use the points in a Geo Engine workflow, we define a [`OgrSource`](https://docs.geoengine.io/operators/ogrsource.html) operator and pass the id of the uploaded points to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points_source_operator = ge.workflow_builder.operators.OgrSource(points_dataset_name)\n",
        "points_source_operator.to_workflow_dict()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we create a workflow that uses [`RasterVectorJoin`](https://docs.geoengine.io/operators/rastervectorjoin.html) operator to join Sentinel-2 data to the points. This operator creates a new column for each band and adds the value of the raster pixel that is closest to each point. The points are provided as input by the [`OgrSource`](https://docs.geoengine.io/operators/ogrsource.html). Additionally the [`RasterVectorJoin`](https://docs.geoengine.io/operators/rastervectorjoin.html) takes up to 8 raster inputs. Here we use the already defined workflows that provide the monthly, cloud-free means of the Sentinel-2 bands and NDVI. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points_with_s2_cloud_free_shift = points_source_operator\n",
        "\n",
        "for month_shift, month_bands in monthly_cloud_free_workflows_shifted.items():\n",
        "    points_with_s2_cloud_free_shift = ge.workflow_builder.operators.RasterVectorJoin(\n",
        "        raster_sources=[x for x in month_bands.values()],\n",
        "        vector_source=points_with_s2_cloud_free_shift, #projected_points,\n",
        "        new_column_names=[f\"{b}_{month_shift}\" for b in month_bands.keys()]\n",
        "    )\n",
        "\n",
        "       \n",
        "points_with_s2_cloud_free_shift.to_workflow_dict()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can register the workflow at the Geo Engine and execute it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "workflow = ge.register_workflow(points_with_s2_cloud_free_shift)\n",
        "workflow"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `ResultDescriptor` of the workflow is a `VectorResultDescriptor`.\n",
        "It includes the description of all the columns that are created by the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "workflow.get_result_descriptor()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To query the workflow we need datetime objects for the start and end of the time interval we are interested in.\n",
        "Since the `TimeShift` operator takes care to generate 8 previous months, we need only need to query the last month of the time interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_dt = datetime(2021, 10, 1, 0, 0, 0)\n",
        "end_dt = start_dt\n",
        "\n",
        "start_dt, end_dt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we query the workflow that attaches the Sentinel-2 data for the area of interest.\n",
        "We use a resolution of 10m, which is the native resolution if the Sentinel-2 bands.\n",
        "The workflow result is transformed into a pandas dataframe automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gp_res = await workflow.vector_stream_into_geopandas(\n",
        "    ge.QueryRectangle(\n",
        "        spatial_bounds=ge.BoundingBox2D(\n",
        "            xmin=xmin,\n",
        "            ymin=ymin,\n",
        "            xmax=xmax,\n",
        "            ymax=ymax,\n",
        "        ),\n",
        "        time_interval=ge.TimeInterval(\n",
        "            start=start_dt,\n",
        "            end=end_dt,\n",
        "        ),\n",
        "        resolution=ge.SpatialResolution(\n",
        "            10.0,\n",
        "            10.0,\n",
        "        ),\n",
        "        srs=\"EPSG:32632\",\n",
        "))\n",
        "\n",
        "# gp_res.to_file(\"gp_res_10_frac1_monthly_use_utm32n_multi_steps_12months_shift.gpkg\", driver=\"GPKG\")\n",
        "gp_res"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train a Random-Forest on the monthly Sentinel-2 data and the NRW crop data\n",
        "\n",
        "Now we can train a Random-Forest on the monthly Sentinel-2 data and the NRW crop data. We use the `sklearn` package for this. But first, we need to prepare the data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we need to replace the nan values in the dataframe with a number that is not part of the dataset. For this example we use 0.\n",
        "This is necessary since the sklearn RF does not support nan values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gp_train_1=gp_res.replace(np.nan, 0)\n",
        "gp_train_1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the training input.\n",
        "This also makes sure that the data is in the correct order when we train the RF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_variable_order = ['B02_0', 'B02_-1', 'B02_-2', 'B02_-3', 'B02_-4',  'B02_-5',  'B02_-6', 'B02_-7', 'B02_-8',\n",
        "        'B03_0', 'B03_-1', 'B03_-2', 'B03_-3', 'B03_-4',  'B03_-5',  'B03_-6', 'B03_-7', 'B03_-8',\n",
        "        'B04_0', 'B04_-1', 'B04_-2', 'B04_-3', 'B04_-4',  'B04_-5',  'B04_-6', 'B04_-7', 'B04_-8',\n",
        "        'B08_0', 'B08_-1', 'B08_-2', 'B08_-3', 'B08_-4',  'B08_-5',  'B08_-6', 'B08_-7', 'B08_-8',\n",
        "        'NDVI_0', 'NDVI_-1', 'NDVI_-2', 'NDVI_-3', 'NDVI_-4',  'NDVI_-5',  'NDVI_-6', 'NDVI_-7', 'NDVI_-8']\n",
        "\n",
        "x_list = gp_train_1[train_variable_order].values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The labels are stored in the column `USE_CODE`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_list = gp_train_1['USE_CODE'].replace(0, 'None')\n",
        "y_list"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we create a train-test split. We use 80% of the data for training and 20% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x_list, y_list, test_size=0.2, random_state=31337, stratify=y_list)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we train the RF on the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = X_train\n",
        "Y = y_train\n",
        "clf = RandomForestClassifier(random_state=1337, class_weight='balanced_subsample', n_estimators=300)\n",
        "clf = clf.fit(X, Y)\n",
        "str(clf)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then create a report of the trained RF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test_predictions = clf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, x_test_predictions, labels=clf.classes_, zero_division=0, digits=3))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, clf.predict(X_test), labels=clf.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
        "disp.plot();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The confusion matrix shows that the class distribution is very imbalanced.\n",
        "This is why the accuracy is so high.\n",
        "The model is very good at predicting the most common classes.\n",
        "This is also visible in the precision and recall scores in the classification report.\n",
        "The precision and recall scores are very high for the most common classes and low for the less common classes. \n",
        "However the F1 score is still high.\n",
        "To imporve the model, we can train on a larger dataset that contains more samples of the less common classes.\n",
        "For this example, we are fine with using the trained RF as is."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Apply the trained Random-Forest to the monthly Sentinel-2 data\n",
        "\n",
        "To generate a raster map of the field-use classes, we can apply the trained RF to the Sentinel-2 data.\n",
        "For this we can create a workflow that generates the same monthly Sentinel-2 data as before, but this time want to apply the trained RF to all the pixels.\n",
        "To do this, we query raster tiles from the Geo Engine and apply the RF to all pixels in each raster tile.\n",
        "The result is a list of xarray data arrays.\n",
        "Normally we would retrain the model on all data, but for demonstration and simplicity we use the model trained on the train/test split."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we specify the query rectangle we are interested in. It is a subeset of the area of interest defined in step 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_dt = datetime(2021, 10, 15, 0, 0, 0)\n",
        "end_dt = datetime(2021, 10, 15, 0, 0, 0)\n",
        "\n",
        "box_size = 512\n",
        "\n",
        "box_center_x = 0.5 * (xmin + xmax)\n",
        "box_center_y = 0.5 * (ymin + ymax)\n",
        "\n",
        "box_x_min = box_center_x - box_size *10\n",
        "box_x_max = box_center_x + box_size *10\n",
        "box_y_min = box_center_y - box_size *10\n",
        "box_y_max = box_center_y + box_size *10\n",
        "\n",
        "query_rect = ge.QueryRectangle(\n",
        "        spatial_bounds=ge.BoundingBox2D(\n",
        "            xmin=box_x_min,\n",
        "            ymin=box_y_min,\n",
        "            xmax=box_x_max,\n",
        "            ymax=box_y_max,\n",
        "        ),\n",
        "        time_interval=ge.TimeInterval(\n",
        "            start=start_dt,\n",
        "            end=end_dt,\n",
        "        ),\n",
        "        resolution=ge.SpatialResolution(\n",
        "            10.0,\n",
        "            10.0,\n",
        "        ),\n",
        "        srs=\"EPSG:32632\",\n",
        ")\n",
        "\n",
        "query_rect"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need a map from category to number. This is necessary since the RF produces class names and we want to store the result as a raster dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "code_id_map = {\n",
        "    'None': 0,\n",
        "    'AF': 1,\n",
        "    'DA': 2,\n",
        "    'EP': 3,\n",
        "    'EW': 4,\n",
        "    'GL': 5,\n",
        "    'GM': 6,\n",
        "    'GT': 7,\n",
        "    'HF': 8,\n",
        "    'HP': 9,\n",
        "    'OE': 10,\n",
        "    'PA': 11,\n",
        "    'SF': 12,\n",
        "    'SL': 13,\n",
        "    'ZP': 14,    \n",
        "}\n",
        "\n",
        "id_code_map = {v: k for k, v in code_id_map.items()}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we query all the workflows in parallel and zip the results together. The result is a list of tiles, where each tile contains the monthly Sentinel-2 data for one band and month.\n",
        "For each tile (list) we apply the RF to the data and store the result as a new tile. The result is a list of tiles, where each tile contains the predicted crop class for one pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_keys = train_variable_order\n",
        "queries = [\n",
        "    ge.register_workflow(monthly_cloud_free_workflows_shifted[int(b.split(\"_\")[1])][b.split(\"_\")[0]]).raster_stream(query_rect) for b in query_keys\n",
        "]\n",
        "\n",
        "res_arrays = []\n",
        "\n",
        "async for tile_stac in zip_longest(*queries):\n",
        "    tiles_as_xarrays = [tile.to_xarray() for tile in tile_stac]\n",
        "\n",
        "    arr_stack = xr.concat(tiles_as_xarrays, dim=\"band\")\n",
        "    arr_stack_2 = arr_stack.transpose(\"y\", \"x\", \"band\")\n",
        "\n",
        "    rf_input = arr_stack_2.values.reshape((box_size * box_size, len(tile_stac)))\n",
        "  \n",
        "    np.nan_to_num(rf_input, copy=False, nan=0, posinf=None, neginf=None)\n",
        "\n",
        "    pred_classes = clf.predict(rf_input)\n",
        "    pred_numbers = np.vectorize(code_id_map.get)(pred_classes)\n",
        "\n",
        "    res_array = pred_numbers.reshape((box_size, box_size))\n",
        "    \n",
        "    da = xr.DataArray(\n",
        "        data=res_array,\n",
        "        dims=[\"y\", \"x\"],\n",
        "        coords=arr_stack_2.coords,\n",
        "        attrs=dict(\n",
        "            description=\"Predicted use.\"\n",
        "        ),\n",
        "    )      \n",
        "\n",
        "    res_arrays.append(da)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now print the result as a image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for arr in res_arrays:\n",
        "    plt.figure()\n",
        "    ax = arr.plot(levels=code_id_map.values(), cmap=\"tab20\")\n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the classification map, we can see that the model is able to generate sound field shapes.\n",
        "It is also visible that the model is able to distinguish between the different field-use classes.\n",
        "When comparing the classification map to the NRW crop data, we can see that the model is able to predict the field-use classes for most of the fields.\n",
        "There are some areas, mainly at the south border of the map, where the model predicted \"Greenland (GL)\".\n",
        "This area is not covered by a field and therefore not labeled in the NRW crop data.\n",
        "However, those areas are Greenland in OpenStreetMap and therefore the model predicted \"Greenland (GL)\" might be correct.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally, we can store the result as a new GeoTiFF file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, arr in enumerate(res_arrays):\n",
        "    arr.rio.to_raster(\n",
        "    f\"arr_{i}.tif\",\n",
        "    tiled=True,  # GDAL: By default striped TIFF files are created. This option can be used to force creation of tiled TIFF files.\n",
        "    windowed=True,  # rioxarray: read & write one window at a time\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
